@misc{zia_surgical_2023,
 abstract = {The ability to automatically detect and track surgical instruments in endoscopic videos can enable transformational interventions. Assessing surgical performance and efficiency, identifying skilled tool use and choreography, and planning operational and logistical aspects of OR resources are just a few of the applications that could benefit. Unfortunately, obtaining the annotations needed to train machine learning models to identify and localize surgical tools is a difficult task. Annotating bounding boxes frame-by-frame is tedious and time-consuming, yet large amounts of data with a wide variety of surgical tools and surgeries must be captured for robust training. Moreover, ongoing annotator training is needed to stay up to date with surgical instrument innovation. In robotic-assisted surgery, however, potentially informative data like timestamps of instrument installation and removal can be programmatically harvested. The ability to rely on tool installation data alone would significantly reduce the workload to train robust tool-tracking models. With this motivation in mind we invited the surgical data science community to participate in the challenge, SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels for machine learning models trained to detect tools and localize them in video frames with bounding boxes. We present the results of this challenge along with many of the team's efforts. We conclude by discussing these results in the broader context of machine learning and surgical data science. The training data used for this challenge consisting of 24,695 video clips with tool presence labels is also being released publicly and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.},
 author = {Zia, Aneeq and Bhattacharyya, Kiran and Liu, Xi and Berniker, Max and Wang, Ziheng and Nespolo, Rogerio and Kondo, Satoshi and Kasai, Satoshi and Hirasawa, Kousuke and Liu, Bo and Austin, David and Wang, Yiheng and Futrega, Michal and Puget, Jean-Francois and Li, Zhenqiang and Sato, Yoichi and Fujii, Ryo and Hachiuma, Ryo and Masuda, Mana and Saito, Hideo and Wang, An and Xu, Mengya and Islam, Mobarakol and Bai, Long and Pang, Winnie and Ren, Hongliang and Nwoye, Chinedu and Sestini, Luca and Padoy, Nicolas and Nielsen, Maximilian and Schüttler, Samuel and Sentker, Thilo and Husseini, Hümeyra and Baltruschat, Ivo and Schmitz, Rüdiger and Werner, René and Matsun, Aleksandr and Farooq, Mugariya and Saaed, Numan and Viera, Jose Renato Restom and Yaqub, Mohammad and Getty, Neil and Xia, Fangfang and Zhao, Zixuan and Duan, Xiaotian and Yao, Xing and Lou, Ange and Yang, Hao and Han, Jintong and Noble, Jack and Wu, Jie Ying and Alshirbaji, Tamer Abdulbaki and Jalal, Nour Aldeen and Arabian, Herag and Ding, Ning and Moeller, Knut and Chen, Weiliang and He, Quan and Bilal, Muhammad and Akinosho, Taofeek and Qayyum, Adnan and Caputo, Massimo and Vohra, Hunaid and Loizou, Michael and Ajayi, Anuoluwapo and Berrou, Ilhem and Niyi-Odumosu, Faatihah and Maier-Hein, Lena and Stoyanov, Danail and Speidel, Stefanie and Jarc, Anthony},
 file = {Zia et al. - 2023 - Surgical tool classification and localization res.pdf:C\:\\Users\łexieli\\Zotero\\storage\\QUQ92QHV\\Zia et al. - 2023 - Surgical tool classification and localization res.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 language = {en},
 month = {May},
 note = {arXiv:2305.07152 [cs]},
 publisher = {arXiv},
 shorttitle = {Surgical tool classification and localization},
 title = {Surgical tool classification and localization: results and methods from the MICCAI 2022 SurgToolLoc challenge},
 url = {http://arxiv.org/abs/2305.07152},
 urldate = {2023-12-14},
 year = {2023}
}
