---
title: 'Surgical tool classification and localization: results and methods from the
  MICCAI 2022 SurgToolLoc challenge'
authors:
- Aneeq Zia
- Kiran Bhattacharyya
- Xi Liu
- Max Berniker
- Ziheng Wang
- Rogerio Nespolo
- Satoshi Kondo
- Satoshi Kasai
- Kousuke Hirasawa
- Bo Liu
- David Austin
- Yiheng Wang
- Michal Futrega
- Jean-Francois Puget
- Zhenqiang Li
- Yoichi Sato
- Ryo Fujii
- Ryo Hachiuma
- Mana Masuda
- Hideo Saito
- An Wang
- Mengya Xu
- Mobarakol Islam
- Long Bai
- Winnie Pang
- Hongliang Ren
- Chinedu Nwoye
- Luca Sestini
- Nicolas Padoy
- Maximilian Nielsen
- Samuel Schüttler
- Thilo Sentker
- Hümeyra Husseini
- Ivo Baltruschat
- Rüdiger Schmitz
- René Werner
- Aleksandr Matsun
- Mugariya Farooq
- Numan Saaed
- Jose Renato Restom Viera
- Mohammad Yaqub
- Neil Getty
- Fangfang Xia
- Zixuan Zhao
- Xiaotian Duan
- Xing Yao
- Ange Lou
- Hao Yang
- Jintong Han
- Jack Noble
- Jie Ying Wu
- Tamer Abdulbaki Alshirbaji
- Nour Aldeen Jalal
- Herag Arabian
- Ning Ding
- Knut Moeller
- Weiliang Chen
- Quan He
- Muhammad Bilal
- Taofeek Akinosho
- Adnan Qayyum
- Massimo Caputo
- Hunaid Vohra
- Michael Loizou
- Anuoluwapo Ajayi
- Ilhem Berrou
- Faatihah Niyi-Odumosu
- Lena Maier-Hein
- Danail Stoyanov
- Stefanie Speidel
- Anthony Jarc
date: '2023-05-01'
publishDate: '2024-01-15T06:08:04.100940Z'
publication_types:
- manuscript
publication: '*arXiv*'
abstract: The ability to automatically detect and track surgical instruments in endoscopic
  videos can enable transformational interventions. Assessing surgical performance
  and efficiency, identifying skilled tool use and choreography, and planning operational
  and logistical aspects of OR resources are just a few of the applications that could
  benefit. Unfortunately, obtaining the annotations needed to train machine learning
  models to identify and localize surgical tools is a difficult task. Annotating bounding
  boxes frame-by-frame is tedious and time-consuming, yet large amounts of data with
  a wide variety of surgical tools and surgeries must be captured for robust training.
  Moreover, ongoing annotator training is needed to stay up to date with surgical
  instrument innovation. In robotic-assisted surgery, however, potentially informative
  data like timestamps of instrument installation and removal can be programmatically
  harvested. The ability to rely on tool installation data alone would significantly
  reduce the workload to train robust tool-tracking models. With this motivation in
  mind we invited the surgical data science community to participate in the challenge,
  SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels for
  machine learning models trained to detect tools and localize them in video frames
  with bounding boxes. We present the results of this challenge along with many of
  the team's efforts. We conclude by discussing these results in the broader context
  of machine learning and surgical data science. The training data used for this challenge
  consisting of 24,695 video clips with tool presence labels is also being released
  publicly and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.
tags:
- Computer Science - Computer Vision and Pattern Recognition
links:
- name: URL
  url: http://arxiv.org/abs/2305.07152
---
