---
title: Learning Important Features Through Propagating Activation Differences
authors:
- Avanti Shrikumar
- Peyton Greenside
- Anshul Kundaje
date: '2019-10-01'
publishDate: '2024-01-15T06:08:03.451361Z'
publication_types:
- manuscript
publication: '*arXiv*'
abstract: 'The purported “black box” nature of neural networks is a barrier to adoption
  in applications where interpretability is essential. Here we present DeepLIFT (Deep
  Learning Important FeaTures), a method for decomposing the output prediction of
  a neural network on a speciﬁc input by backpropagating the contributions of all
  neurons in the network to every feature of the input. DeepLIFT compares the activation
  of each neuron to its ‘reference activation’ and assigns contribution scores according
  to the difference. By optionally giving separate consideration to positive and negative
  contributions, DeepLIFT can also reveal dependencies which are missed by other approaches.
  Scores can be computed efﬁciently in a single backward pass. We apply DeepLIFT to
  models trained on MNIST and simulated genomic data, and show signiﬁcant advantages
  over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:
  bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.'
tags:
- Computer Science - Computer Vision and Pattern Recognition
- Computer Science - Machine Learning
- Computer Science - Neural and Evolutionary Computing
links:
- name: URL
  url: http://arxiv.org/abs/1704.02685
---
