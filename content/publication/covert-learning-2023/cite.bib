@misc{covert_learning_2023,
 abstract = {Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model’s dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we ﬁrst leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we ﬁnd that our approach provides more accurate explanations than existing methods for ViTs.},
 annote = {Comment: ICLR 2023 camera-ready},
 author = {Covert, Ian and Kim, Chanwoo and Lee, Su-In},
 file = {Covert et al. - 2023 - Learning to Estimate Shapley Values with Vision Tr.pdf:C\:\\Users\łexieli\\Zotero\\storage\\RC39Q6FI\\Covert et al. - 2023 - Learning to Estimate Shapley Values with Vision Tr.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:2206.05282 [cs]},
 publisher = {arXiv},
 title = {Learning to Estimate Shapley Values with Vision Transformers},
 url = {http://arxiv.org/abs/2206.05282},
 urldate = {2023-12-14},
 year = {2023}
}
