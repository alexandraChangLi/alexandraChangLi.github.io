@article{wu_cross-modal_2021,
 abstract = {Purpose Multi- and cross-modal learning consolidates information from multiple data sources which may offer a holistic representation of complex scenarios. Cross-modal learning is particularly interesting, because synchronized data streams are immediately useful as self-supervisory signals. The prospect of achieving self-supervised continual learning in surgical robotics is exciting as it may enable lifelong learning that adapts to different surgeons and cases, ultimately leading to a more general machine understanding of surgical processes.
Methods We present a learning paradigm using synchronous video and kinematics from robot-mediated surgery. Our approach relies on an encoder–decoder network that maps optical ﬂow to the corresponding kinematics sequence. Clustering on the latent representations reveals meaningful groupings for surgeon gesture and skill level. We demonstrate the generalizability of the representations on the JIGSAWS dataset by classifying skill and gestures on tasks not used for training.
Results For tasks seen in training, we report a 59 to 70% accuracy in surgical gestures classiﬁcation. On tasks beyond the training setup, we note a 45 to 65% accuracy. Qualitatively, we ﬁnd that unseen gestures form clusters in the latent space of novice actions, which may enable the automatic identiﬁcation of novel interactions in a lifelong learning scenario.
Conclusion From predicting the synchronous kinematics sequence, optical ﬂow representations of surgical scenes emerge that separate well even for new tasks that the model had not seen before. While the representations are useful immediately for a variety of tasks, the self-supervised learning paradigm may enable research in lifelong and user-speciﬁc learning.},
 author = {Wu, Jie Ying and Tamhane, Aniruddha and Kazanzides, Peter and Unberath, Mathias},
 doi = {10.1007/s11548-021-02343-y},
 file = {Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:C\:\\Users\łexieli\\Zotero\\storage\\XFBNMNKX\\Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:application/pdf},
 issn = {1861-6410, 1861-6429},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 language = {en},
 month = {May},
 number = {5},
 pages = {779--787},
 title = {Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery},
 url = {https://link.springer.com/10.1007/s11548-021-02343-y},
 urldate = {2023-12-14},
 volume = {16},
 year = {2021}
}
