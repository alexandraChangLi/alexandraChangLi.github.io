---
title: 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers
  Models'
authors:
- Benjamin Hoover
- Hendrik Strobelt
- Sebastian Gehrmann
date: '2019-10-01'
publishDate: '2024-01-15T06:08:03.461897Z'
publication_types:
- manuscript
publication: '*arXiv*'
abstract: Large language models can produce powerful contextual representations that
  lead to improvements across many NLP tasks. Since these models are typically guided
  by a sequence of learned self attention mechanisms and may comprise undesired inductive
  biases, it is paramount to be able to explore what the attention has learned. While
  static analyses of these models lead to targeted insights, interactive tools are
  more dynamic and can help humans better gain an intuition for the modelinternal
  reasoning process. We present EXBERT, an interactive tool named after the popular
  BERT language model, that provides insights into the meaning of the contextual representations
  by matching a human-speciÔ¨Åed input to similar contexts in a large annotated dataset.
  By aggregating the annotations of the matching similar contexts, EXBERT helps intuitively
  explain what each attention-head has learned.
tags:
- Computer Science - Machine Learning
- Computer Science - Computation and Language
links:
- name: URL
  url: http://arxiv.org/abs/1910.05276
---
