@misc{zhu_3d-vista_2023,
 abstract = {3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.},
 author = {Zhu, Ziyu and Ma, Xiaojian and Chen, Yixin and Deng, Zhidong and Huang, Siyuan and Li, Qing},
 file = {Zhu et al. - 2023 - 3D-VisTA Pre-trained Transformer for 3D Vision an.pdf:C\:\\Users\Å‚exieli\\Zotero\\storage\\TZEPI6ZY\\Zhu et al. - 2023 - 3D-VisTA Pre-trained Transformer for 3D Vision an.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 language = {en},
 month = {August},
 note = {arXiv:2308.04352 [cs]},
 publisher = {arXiv},
 shorttitle = {3D-VisTA},
 title = {3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment},
 url = {http://arxiv.org/abs/2308.04352},
 urldate = {2023-12-14},
 year = {2023}
}
