---
title: A Unified Approach to Interpreting Model Predictions
authors:
- Scott Lundberg
- Su-In Lee
date: '2017-11-01'
publishDate: '2024-01-15T06:08:03.529608Z'
publication_types:
- manuscript
publication: '*arXiv*'
abstract: 'Understanding why a model makes a certain prediction can be as crucial
  as the prediction’s accuracy in many applications. However, the highest accuracy
  for large modern datasets is often achieved by complex models that even experts
  struggle to interpret, such as ensemble or deep learning models, creating a tension
  between accuracy and interpretability. In response, various methods have recently
  been proposed to help users interpret the predictions of complex models, but it
  is often unclear how these methods are related and when one method is preferable
  over another. To address this problem, we present a uniﬁed framework for interpreting
  predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an
  importance value for a particular prediction. Its novel components include: (1)
  the identiﬁcation of a new class of additive feature importance measures, and (2)
  theoretical results showing there is a unique solution in this class with a set
  of desirable properties. The new class uniﬁes six existing methods, notable because
  several recent methods in the class lack the proposed desirable properties. Based
  on insights from this uniﬁcation, we present new methods that show improved computational
  performance and/or better consistency with human intuition than previous approaches.'
tags:
- Computer Science - Machine Learning
- Computer Science - Artificial Intelligence
- Statistics - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/1705.07874
---
