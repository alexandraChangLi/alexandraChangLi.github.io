
@misc{shrikumar_learning_2019,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a speciﬁc input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efﬁciently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show signiﬁcant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = oct,
	year = {2019},
	note = {arXiv:1704.02685 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Updated to include changes present in the ICML camera-ready paper, and other small corrections},
	annote = {This paper define a reference activation baseline and compare model’s gradient based on the activation delta. 
It defines a ‘summation-to-delta’ property, and deduce the chain rule for multipliers, tackling two problems including discontinuous gradients and saturation problems.
Regarding different types of layers, several tricks can be adopted to adjust specific scenarios.
This paper is elegant in its math form while I did not closely inspect its experiments.
},
	file = {Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\TNS53LPG\\Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf:application/pdf},
}

@misc{hoover_exbert_2019,
	title = {{exBERT}: {A} {Visual} {Analysis} {Tool} to {Explore} {Learned} {Representations} in {Transformers} {Models}},
	shorttitle = {{exBERT}},
	url = {http://arxiv.org/abs/1910.05276},
	abstract = {Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the modelinternal reasoning process. We present EXBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-speciﬁed input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, EXBERT helps intuitively explain what each attention-head has learned.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Hoover, Benjamin and Strobelt, Hendrik and Gehrmann, Sebastian},
	month = oct,
	year = {2019},
	note = {arXiv:1910.05276 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Summary: This paper propose a visualization tool focusing on BERT model.
It claims that it utilize linguistic annotations, interactive masking and nearest neighbor search. But I think its highlight lies in the nearest neighbor search and utilize various types of heads and embeddings for calculation.
The visualization results support their claims while whether it can be transferred to other types of transformers can be questionable, since not all data processed by transformer has this direct detailed meaning. For example, when processing point clouds, there are individual points that need to be considered.
},
	file = {Hoover et al. - 2019 - exBERT A Visual Analysis Tool to Explore Learned .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NFBKFBPQ\\Hoover et al. - 2019 - exBERT A Visual Analysis Tool to Explore Learned .pdf:application/pdf},
}

@misc{hesse_fast_2021,
	title = {Fast {Axiomatic} {Attribution} for {Neural} {Networks}},
	url = {http://arxiv.org/abs/2111.07668},
	abstract = {Mitigating the dependence on spurious correlations present in the training dataset is a quickly emerging and important topic of deep learning. Recent approaches include priors on the feature attribution of a deep neural network (DNN) into the training process to reduce the dependence on unwanted features. However, until now one needed to trade off high-quality attributions, satisfying desirable axioms, against the time required to compute them. This in turn either led to long training times or ineffective attribution priors. In this work, we break this trade-off by considering a special class of efﬁciently axiomatically attributable DNNs for which an axiomatic feature attribution can be computed with only a single forward/backward pass. We formally prove that nonnegatively homogeneous DNNs, here termed X -DNNs, are efﬁciently axiomatically attributable and show that they can be effortlessly constructed from a wide range of regular DNNs by simply removing the bias term of each layer. Various experiments demonstrate the advantages of X -DNNs, beating state-of-the-art generic attribution methods on regular DNNs for training with attribution priors.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Hesse, Robin and Schaub-Meyer, Simone and Roth, Stefan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.07668 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: To appear at NeurIPS*2021. Project page and code: https://visinf.github.io/fast-axiomatic-attribution},
	annote = {This work remove the bias term in DNN’s training and propose that the simplified DNN obtain various properties. Furthermore, the attribution (input * gradient?) can be computed in one propagation.
There are some experiments demonstrating that reducing the bias term does not affect the model’s performance much. Also, there are visualizations of the results.
I think the major limitation comes from the restricted set of the network and reducing the bias term. Also, evaluating the performance of the model without bias term is computational-costly. I believe that it requires re-training.
},
	file = {Hesse et al. - 2021 - Fast Axiomatic Attribution for Neural Networks.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\TCZIPWLU\\Hesse et al. - 2021 - Fast Axiomatic Attribution for Neural Networks.pdf:application/pdf},
}

@article{ying_gnnexplainer_nodate,
	title = {{GNNExplainer}: {Generating} {Explanations} for {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GNNEXPLAINER, the ﬁrst general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNEXPLAINER identiﬁes a compact subgraph structure and a small subset of node features that have a crucial role in GNN’s prediction. Further, GNNEXPLAINER can generate consistent and concise explanations for an entire class of instances. We formulate GNNEXPLAINER as an optimization task that maximizes the mutual information between a GNN’s prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0\% in explanation accuracy. GNNEXPLAINER provides a variety of beneﬁts, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
	language = {en},
	author = {Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
	annote = {This paper utilizes masks to select sub-graphs for explaining GNN classifier’s outcome. Instead of explaining, I would like to describe it as: ‘identifying critical patterns’ that resembles certain classes.
It is done through minimizing a certain loss, aiming at training a mask, M (I think M is the goal of the training but did not look for specific proofs).
I think this is interesting. But I cannot see the potential in applying it to quant tasks, since the demonstrations provided in the experiments sections are largely chemical molecules that diverse in graph nodes and paths among nodes. However, among quant features, I do not think that they have various innate connections.
},
	file = {Ying et al. - GNNExplainer Generating Explanations for Graph Ne.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\UL2LV9DP\\Ying et al. - GNNExplainer Generating Explanations for Graph Ne.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2023-12-14},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {336--359},
	annote = {Comment: This version was published in International Journal of Computer Vision (IJCV) in 2019; A previous version of the paper was published at International Conference on Computer Vision (ICCV'17)},
	annote = {This work presents a visualization method. It focuses on extracting the gradients in the propagation process. 
Via various visualizations, the results show that this method is robust and can demonstrate various hidden representations.
We can adopt a previous method to name a neuron and apply grad-cam to select neurons for explanations.

The correlation conducted on the image-occlusion results also demonstrate its strong performance. 
This method can also be generalized to Res-Net based models. Also, in the model’s method figure, we notice that we can adopt RNN/LSTM on the layers adjacent to the previous CNN.

Overall, I think it is a good paper.
},
	file = {Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\W4M9HZTD\\Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf},
}

@misc{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the ﬂow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a uniﬁed framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classiﬁcation, using various network architectures.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {arXiv:1711.06104 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2018},
	annote = {Overall, it is about how to evaluate these evaluation methods utilizing the sensitivity-n formula that it proposes.
I think it is a little bit of inside-the-box since the experiments they conducted has a lot of constraints and I would like to see more qualitative analysis on detailed scenario.
The effect of sensitivity-n’s proof is not sufficient I think, since they only look at the correlation and the results. 
I think it is a bad paper on Christmas. Maybe after reading it a few days later can provide me with a better insight.
},
	file = {Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\JRR2NSAY\\Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf:application/pdf},
}

@misc{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisﬁed by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modiﬁcation to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jun,
	year = {2017},
	note = {arXiv:1703.01365 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {This paper proposes two requirements that evaluation methods should satisfy and proposes that there are no methods that satisfy these two qualities. I doubt the second quality to be reasonable.
It argues that everything can be traced back into the gradient calculation method. Instead of the chain rule, this paper proposes to calculate gradients using integral calculation.
It is a very interesting approach while I doubt its relation with its proposals.
Also, it would be nice to demonstrate comparisons with other methods.
},
	file = {Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\HFLR5ZLQ\\Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:application/pdf},
}

@misc{janizek_explaining_2020,
	title = {Explaining {Explanations}: {Axiomatic} {Feature} {Interactions} for {Deep} {Networks}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/2002.04138},
	abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model’s prediction on a given input. However, for many tasks, simply knowing which features were important to a model’s prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians2: an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a speciﬁc architecture or class of neural network. Additionally, we ﬁnd that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Janizek, Joseph D. and Sturmfels, Pascal and Lee, Su-In},
	month = jun,
	year = {2020},
	note = {arXiv:2002.04138 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {This paper is gold. It propose to take the second-order derivative of the integrated gradient, into the integrated hessian matrix. These scores calculated from the hessian matrix correspond to two features’ interactions.
The experiment results are promising and have wide application.
I did not inspect the detailed math deduction part but intuitively these theorems seem correct.
It has a wide further application.
},
	file = {Janizek et al. - 2020 - Explaining Explanations Axiomatic Feature Interac.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\PT6I7RF7\\Janizek et al. - 2020 - Explaining Explanations Axiomatic Feature Interac.pdf:application/pdf},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {This paper is gold. It proposes a local model approximation method and a submodular picking process for features to demonstrate what plays an important part in models’ decision making process. Greedy algorithm is used to solve to NP-optimization problem.
Various human-in-the-loop experiments demonstrates the effectiveness of this method. 
},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\5AQHKANK\\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@misc{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv:1705.07874 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: To appear in NIPS 2017},
	annote = {This paper proposes various corollaries and theorems to prove that existing explanation methods have some feasible properties, thus can be further integrate into the SHAP value’s calculation.
It can be seen from the experiments that the results are good. However I wonder the applicability scope of these methods for these preliminary theorems.
Overall, a good paper.
},
	file = {Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\3TUS9SVK\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@misc{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {arXiv:1706.03825 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages},
	annote = {This is a simple work. It hypothesizes that noisy sentitivity maps are due to noisy gradients. It converts the noisy(defined by human) sensitivity maps via applying gaussian noise and calculate the average. The experimental results on pictures demonstrate its effectiveness observing the direct visualizations.
However, I think sometimes more visualizations can be done. And I question its effectiveness based on the experiments on a few samples. I wonder if there are other kind of metrics on evaluating the results. 
And this method does not provide how this technique assist interpretability of the models. The experiments are limited.

},
	file = {Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\L5HMSKFL\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf},
}

@misc{misra_end--end_2021,
	title = {An {End}-to-{End} {Transformer} {Model} for {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2109.08141},
	abstract = {We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3Dspeciﬁc inductive biases, 3DETR requires minimal modiﬁcations to the vanilla Transformer block. Speciﬁcally, we ﬁnd that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3Dspeciﬁc operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5\%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
	month = sep,
	year = {2021},
	note = {arXiv:2109.08141 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at ICCV 2021},
	file = {Misra et al. - 2021 - An End-to-End Transformer Model for 3D Object Dete.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\ID9498TN\\Misra et al. - 2021 - An End-to-End Transformer Model for 3D Object Dete.pdf:application/pdf},
}

@misc{sun_3d-gpt_2023,
	title = {{3D}-{GPT}: {Procedural} {3D} {Modeling} with {Large} {Language} {Models}},
	shorttitle = {{3D}-{GPT}},
	url = {http://arxiv.org/abs/2310.12945},
	abstract = {In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models (LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Sun, Chunyi and Han, Junlin and Deng, Weijian and Wang, Xinlong and Qin, Zishan and Gould, Stephen},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12945 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	annote = {Comment: Project page: https://chuny1.github.io/3DGPT/3dgpt.html},
	file = {Sun et al. - 2023 - 3D-GPT Procedural 3D Modeling with Large Language.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\H8PEPNYL\\Sun et al. - 2023 - 3D-GPT Procedural 3D Modeling with Large Language.pdf:application/pdf},
}

@misc{hong_3d-llm_2023,
	title = {{3D}-{LLM}: {Injecting} the {3D} {World} into {Large} {Language} {Models}},
	shorttitle = {{3D}-{LLM}},
	url = {http://arxiv.org/abs/2307.12981},
	abstract = {Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multiview images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12981 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Project Page: : https://vis-www.cs.umass.edu/3dllm/},
	file = {Hong et al. - 2023 - 3D-LLM Injecting the 3D World into Large Language.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\QMEE4ZBU\\Hong et al. - 2023 - 3D-LLM Injecting the 3D World into Large Language.pdf:application/pdf},
}

@misc{zhu_3d-vista_2023,
	title = {{3D}-{VisTA}: {Pre}-trained {Transformer} for {3D} {Vision} and {Text} {Alignment}},
	shorttitle = {{3D}-{VisTA}},
	url = {http://arxiv.org/abs/2308.04352},
	abstract = {3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhu, Ziyu and Ma, Xiaojian and Chen, Yixin and Deng, Zhidong and Huang, Siyuan and Li, Qing},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04352 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhu et al. - 2023 - 3D-VisTA Pre-trained Transformer for 3D Vision an.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\TZEPI6ZY\\Zhu et al. - 2023 - 3D-VisTA Pre-trained Transformer for 3D Vision an.pdf:application/pdf},
}

@misc{zhang_prompt_2023,
	title = {Prompt, {Generate}, then {Cache}: {Cascade} of {Foundation} {Models} makes {Strong} {Few}-shot {Learners}},
	shorttitle = {Prompt, {Generate}, then {Cache}},
	url = {http://arxiv.org/abs/2303.02151},
	abstract = {Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance beneﬁted from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP’s language-contrastive knowledge, DINO’s vision-contrastive knowledge, DALL-E’s visiongenerative knowledge, and GPT-3’s language-generative knowledge. Speciﬁcally, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-ofthe-art for few-shot classiﬁcation. Code is available at https://github.com/ZrrSkywalker/CaFo.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhang, Renrui and Hu, Xiangfei and Li, Bohao and Huang, Siyuan and Deng, Hanqiu and Li, Hongsheng and Qiao, Yu and Gao, Peng},
	month = mar,
	year = {2023},
	note = {arXiv:2303.02151 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: Accepted by CVPR 2023. Code is available at https://github.com/ZrrSkywalker/CaFo. arXiv admin note: substantial text overlap with arXiv:2209.12255},
	file = {Zhang et al. - 2023 - Prompt, Generate, then Cache Cascade of Foundatio.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\KMQ6CNEH\\Zhang et al. - 2023 - Prompt, Generate, then Cache Cascade of Foundatio.pdf:application/pdf},
}

@inproceedings{lu_open-vocabulary_2023,
	address = {Vancouver, BC, Canada},
	title = {Open-{Vocabulary} {Point}-{Cloud} {Object} {Detection} without {3D} {Annotation}},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203153/},
	doi = {10.1109/CVPR52729.2023.00121},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lu, Yuheng and Xu, Chenfeng and Wei, Xiaobao and Xie, Xiaodong and Tomizuka, Masayoshi and Keutzer, Kurt and Zhang, Shanghang},
	month = jun,
	year = {2023},
	pages = {1190--1199},
	file = {Lu et al. - 2023 - Open-Vocabulary Point-Cloud Object Detection witho.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\BNCVGWQS\\Lu et al. - 2023 - Open-Vocabulary Point-Cloud Object Detection witho.pdf:application/pdf},
}

@misc{zhang_pointclip_2021,
	title = {{PointCLIP}: {Point} {Cloud} {Understanding} by {CLIP}},
	shorttitle = {{PointCLIP}},
	url = {http://arxiv.org/abs/2112.02413},
	abstract = {Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale imagetext pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing PointCLIP, which conducts alignment between CLIPencoded point cloud and 3D category texts. Speciﬁcally, we encode a point cloud by projecting it into multi-view depth maps without rendering, and aggregate the view-wise zeroshot prediction to achieve knowledge transfer from 2D to 3D. On top of that, we design an inter-view adapter to better extract the global feature and adaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in 2D. By just ﬁne-tuning the lightweight adapter in the few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the complementary property between PointCLIP and classical 3D-supervised networks. By simple ensembling, PointCLIP boosts baseline’s performance and even surpasses state-of-the-art models. Therefore, PointCLIP is a promising alternative for effective 3D point cloud understanding via CLIP under low resource cost and data regime. We conduct thorough experiments on widely-adopted ModelNet10, ModelNet40 and the challenging ScanObjectNN to demonstrate the effectiveness of PointCLIP. The code is released at https: //github.com/ZrrSkywalker/PointCLIP.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng},
	month = dec,
	year = {2021},
	note = {arXiv:2112.02413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Open sourced, Code and Model Available},
	file = {Zhang et al. - 2021 - PointCLIP Point Cloud Understanding by CLIP.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\RNHRLG54\\Zhang et al. - 2021 - PointCLIP Point Cloud Understanding by CLIP.pdf:application/pdf},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize ﬁne-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efﬁciently and robustly. In particular, results signiﬁcantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv:1706.02413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\GRK9ELPW\\Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:application/pdf},
}

@misc{ding_pla_2023,
	title = {{PLA}: {Language}-{Driven} {Open}-{Vocabulary} {3D} {Scene} {Understanding}},
	shorttitle = {{PLA}},
	url = {http://arxiv.org/abs/2211.16312},
	abstract = {Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated label space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired imagetext data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pretrained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows explicitly associating 3D and semantic-rich captions. Further, to foster coarse-to-fine visual-semantic representation learning from captions, we design hierarchical 3Dcaption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms baseline methods by 25.8\% ∼ 44.7\% hIoU and 14.5\% ∼ 50.4\% hAP50 in open-vocabulary semantic and instance segmentation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Ding, Runyu and Yang, Jihan and Xue, Chuhui and Zhang, Wenqing and Bai, Song and Qi, Xiaojuan},
	month = mar,
	year = {2023},
	note = {arXiv:2211.16312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR2023},
	file = {Ding et al. - 2023 - PLA Language-Driven Open-Vocabulary 3D Scene Unde.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NX89PVRJ\\Ding et al. - 2023 - PLA Language-Driven Open-Vocabulary 3D Scene Unde.pdf:application/pdf},
}

@inproceedings{zhang_growsp_2023,
	address = {Vancouver, BC, Canada},
	title = {{GrowSP}: {Unsupervised} {Semantic} {Segmentation} of {3D} {Point} {Clouds}},
	isbn = {9798350301298},
	shorttitle = {{GrowSP}},
	url = {https://ieeexplore.ieee.org/document/10203698/},
	doi = {10.1109/CVPR52729.2023.01690},
	abstract = {We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on multiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fullysupervised PointNet. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Zihui and Yang, Bo and Wang, Bing and Li, Bo},
	month = jun,
	year = {2023},
	pages = {17619--17629},
	file = {Zhang et al. - 2023 - GrowSP Unsupervised Semantic Segmentation of 3D P.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\YUU9T8VI\\Zhang et al. - 2023 - GrowSP Unsupervised Semantic Segmentation of 3D P.pdf:application/pdf},
}

@misc{fu_shapecrafter_2023,
	title = {{ShapeCrafter}: {A} {Recursive} {Text}-{Conditioned} {3D} {Shape} {Generation} {Model}},
	shorttitle = {{ShapeCrafter}},
	url = {http://arxiv.org/abs/2207.09446},
	abstract = {We present ShapeCrafter, a neural network for recursive text-conditioned 3D shape generation. Existing methods that generate text-conditioned 3D shapes consume an entire text prompt to generate a 3D shape in a single step. However, humans tend to describe shapes recursively—we may start with an initial description and progressively add details based on intermediate results. To capture this recursive process, we introduce a method to generate a 3D shape distribution, conditioned on an initial phrase, that gradually evolves as more phrases are added. Since existing datasets are insufﬁcient for training under this approach, we present Text2Shape++, a large dataset of 369K shape–text pairs that supports recursive shape generation. To capture local details that are often used to reﬁne shape descriptions, we build upon vector-quantized deep implicit functions that generate a distribution of high-quality shapes. Results show that our method can generate shapes consistent with text descriptions, and shapes evolve gradually as more phrases are added. Our method supports shape editing, extrapolation, and can enable new applications in human–machine collaboration for creative design.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Fu, Rao and Zhan, Xiao and Chen, Yiwen and Ritchie, Daniel and Sridhar, Srinath},
	month = apr,
	year = {2023},
	note = {arXiv:2207.09446 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented at the Advances in Neural Information Processing Systems (NeurIPS) 2022},
	file = {Fu et al. - 2023 - ShapeCrafter A Recursive Text-Conditioned 3D Shap.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\M8BQB3J3\\Fu et al. - 2023 - ShapeCrafter A Recursive Text-Conditioned 3D Shap.pdf:application/pdf},
}

@misc{sanghi_clip-sculptor_2023,
	title = {{CLIP}-{Sculptor}: {Zero}-{Shot} {Generation} of {High}-{Fidelity} and {Diverse} {Shapes} from {Natural} {Language}},
	shorttitle = {{CLIP}-{Sculptor}},
	url = {http://arxiv.org/abs/2211.01427},
	abstract = {Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP’s image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Sanghi, Aditya and Fu, Rao and Liu, Vivian and Willis, Karl and Shayani, Hooman and Khasahmadi, Amir Hosein and Sridhar, Srinath and Ritchie, Daniel},
	month = may,
	year = {2023},
	note = {arXiv:2211.01427 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at Conference on Computer Vision and Pattern Recognition 2023(CVPR2023)},
	file = {Sanghi et al. - 2023 - CLIP-Sculptor Zero-Shot Generation of High-Fideli.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\ZLBDJIHC\\Sanghi et al. - 2023 - CLIP-Sculptor Zero-Shot Generation of High-Fideli.pdf:application/pdf},
}

@misc{sen_scarp_2023,
	title = {{SCARP}: {3D} {Shape} {Completion} in {ARbitrary} {Poses} for {Improved} {Grasping}},
	shorttitle = {{SCARP}},
	url = {http://arxiv.org/abs/2301.07213},
	abstract = {Recovering full 3D shapes from partial observations is a challenging task that has been extensively addressed in the computer vision community. Many deep learning methods tackle this problem by training 3D shape generation networks to learn a prior over the full 3D shapes. In this training regime, the methods expect the inputs to be in a fixed canonical form, without which they fail to learn a valid prior over the 3D shapes. We propose SCARP, a model that performs Shape Completion in ARbitrary Poses. Given a partial pointcloud of an object, SCARP learns a disentangled feature representation of pose and shape by relying on rotationally equivariant pose features and geometric shape features trained using a multi-tasking objective. Unlike existing methods that depend on an external canonicalization, SCARP performs canonicalization, pose estimation, and shape completion in a single network, improving the performance by 45\% over the existing baselines. In this work, we use SCARP for improving grasp proposals on tabletop objects. By completing partial tabletop objects directly in their observed poses, SCARP enables a SOTA grasp proposal network improve their proposals by 71.2\% on partial shapes. Project page: https://bipashasen.github.io/scarp},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Sen, Bipasha and Agarwal, Aditya and Singh, Gaurav and B., Brojeshwar and Sridhar, Srinath and Krishna, Madhava},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07213 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Accepted at ICRA 2023},
	file = {Sen et al. - 2023 - SCARP 3D Shape Completion in ARbitrary Poses for .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\6NKSP668\\Sen et al. - 2023 - SCARP 3D Shape Completion in ARbitrary Poses for .pdf:application/pdf},
}

@misc{jones_shred_2022,
	title = {{SHRED}: {3D} {Shape} {Region} {Decomposition} with {Learned} {Local} {Operations}},
	shorttitle = {{SHRED}},
	url = {http://arxiv.org/abs/2206.03480},
	abstract = {We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a 3D point cloud as input and uses learned local operations to produce a segmentation that approximates fine-grained part instances. We endow SHRED with three decomposition operations: splitting regions, fixing the boundaries between regions, and merging regions together. Modules are trained independently and locally, allowing SHRED to generate high-quality segmentations for categories not seen during training. We train and evaluate SHRED with fine-grained segmentations from PartNet; using its mergethreshold hyperparameter, we show that SHRED produces segmentations that better respect ground-truth annotations compared with baseline methods, at any desired decomposition granularity. Finally, we demonstrate that SHRED is useful for downstream applications, out-performing all baselines on zero-shot fine-grained part instance segmentation and few-shot finegrained semantic segmentation when combined with methods that learn to label shape regions. CCS Concepts: • Computing methodologies → Shape analysis; Neural networks.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Jones, R. Kenny and Habib, Aalia and Ritchie, Daniel},
	month = oct,
	year = {2022},
	note = {arXiv:2206.03480 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	annote = {Comment: SIGGRAPH ASIA 2022},
	file = {Jones et al. - 2022 - SHRED 3D Shape Region Decomposition with Learned .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\4AWUBCCX\\Jones et al. - 2022 - SHRED 3D Shape Region Decomposition with Learned .pdf:application/pdf},
}

@misc{xu_unsupervised_2023,
	title = {Unsupervised {3D} {Shape} {Reconstruction} by {Part} {Retrieval} and {Assembly}},
	url = {http://arxiv.org/abs/2303.01999},
	abstract = {Representing a 3D shape with a set of primitives can aid perception of structure, improve robotic object manipulation, and enable editing, stylization, and compression of 3D shapes. Existing methods either use simple parametric primitives or learn a generative shape space of parts. Both have limitations: parametric primitives lead to coarse approximations, while learned parts offer too little control over the decomposition. We instead propose to decompose shapes using a library of 3D parts provided by the user, giving full control over the choice of parts. The library can contain parts with high-quality geometry that are suitable for a given category, resulting in meaningful decompositions with clean geometry. The type of decomposition can also be controlled through the choice of parts in the library. Our method works via a self-supervised approach that iteratively retrieves parts from the library and reﬁnes their placements. We show that this approach gives higher reconstruction accuracy and more desirable decompositions than existing approaches. Additionally, we show how the decomposition can be controlled through the part library by using different part libraries to reconstruct the same shapes.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Xu, Xianghao and Guerrero, Paul and Fisher, Matthew and Chaudhuri, Siddhartha and Ritchie, Daniel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01999 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: CVPR 2023},
	file = {Xu et al. - 2023 - Unsupervised 3D Shape Reconstruction by Part Retri.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NTN82B38\\Xu et al. - 2023 - Unsupervised 3D Shape Reconstruction by Part Retri.pdf:application/pdf},
}

@misc{khan_edge-aware_2021,
	title = {Edge-aware {Bidirectional} {Diffusion} for {Dense} {Depth} {Estimation} from {Light} {Fields}},
	url = {http://arxiv.org/abs/2107.02967},
	abstract = {We present an algorithm to estimate fast and accurate depth maps from light fields via a sparse set of depth edges and gradients. Our proposed approach is based around the idea that true depth edges are more sensitive than texture edges to local constraints, and so they can be reliably disambiguated through a bidirectional diffusion process. First, we use epipolar-plane images to estimate sub-pixel disparity at a sparse set of pixels. To find sparse points efficiently, we propose an entropy-based refinement approach to a line estimate from a limited set of oriented filter banks. Next, to estimate the diffusion direction away from sparse points, we optimize constraints at these points via our bidirectional diffusion method. This resolves the ambiguity of which surface the edge belongs to and reliably separates depth from texture edges, allowing us to diffuse the sparse set in a depth-edge and occlusion-aware manner to obtain accurate dense depth maps.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Khan, Numair and Kim, Min H. and Tompkin, James},
	month = jul,
	year = {2021},
	note = {arXiv:2107.02967 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project webpage: http://visual.cs.brown.edu/lightfielddepth},
	file = {Khan et al. - 2021 - Edge-aware Bidirectional Diffusion for Dense Depth.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\J3ANS36M\\Khan et al. - 2021 - Edge-aware Bidirectional Diffusion for Dense Depth.pdf:application/pdf},
}

@inproceedings{goel_shape_2020,
	address = {Fukuoka, Japan},
	title = {Shape from {Tracing}: {Towards} {Reconstructing} {3D} {Object} {Geometry} and {SVBRDF} {Material} from {Images} via {Differentiable} {Path} {Tracing}},
	isbn = {978-1-72818-128-8},
	shorttitle = {Shape from {Tracing}},
	url = {https://ieeexplore.ieee.org/document/9320395/},
	doi = {10.1109/3DV50981.2020.00129},
	abstract = {Reconstructing object geometry and material from multiple views typically requires optimization. Differentiable path tracing is an appealing framework as it can reproduce complex appearance effects. However, it is difﬁcult to use due to high computational cost. In this paper, we explore how to use differentiable ray tracing to reﬁne an initial coarse mesh and per-mesh-facet material representation. In simulation, we ﬁnd that it is possible to reconstruct ﬁne geometric and material detail from low resolution input views, allowing high-quality reconstructions in a few hours despite the expense of path tracing. The reconstructions successfully disambiguate shading, shadow, and global illumination effects such as diffuse interreﬂection from material properties. We demonstrate the impact of different geometry initializations, including space carving, multi-view stereo, and 3D neural networks. Finally, with input captured using smartphone video and a consumer 360◦ camera for lighting estimation, we also show how to reﬁne initial reconstructions of real-world objects in unconstrained environments.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2020 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Goel, Purvi and Cohen, Loudon and Guesman, James and Thamizharasan, Vikas and Tompkin, James and Ritchie, Daniel},
	month = nov,
	year = {2020},
	pages = {1186--1195},
	file = {Goel et al. - 2020 - Shape from Tracing Towards Reconstructing 3D Obje.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\RY76XBKN\\Goel et al. - 2020 - Shape from Tracing Towards Reconstructing 3D Obje.pdf:application/pdf},
}

@article{hebert_generative_nodate,
	title = {Generative {Modeling} for {Multi}-task {Visual} {Learning}},
	abstract = {Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, consistently outperforming state-of-the-art multi-task approaches in different sample-size regimes.},
	language = {en},
	author = {Hebert, Martial and Wang, Yu-Xiong},
	file = {Hebert and Wang - Generative Modeling for Multi-task Visual Learning.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\ZU4NZPT5\\Hebert and Wang - Generative Modeling for Multi-task Visual Learning.pdf:application/pdf},
}

@inproceedings{alwala_pretrain_2022,
	address = {New Orleans, LA, USA},
	title = {Pretrain, {Self}-train, {Distill}: {A} simple recipe for {Supersizing} {3D} {Reconstruction}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Pretrain, {Self}-train, {Distill}},
	url = {https://ieeexplore.ieee.org/document/9880060/},
	doi = {10.1109/CVPR52688.2022.00375},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Alwala, Kalyan Vasudev and Gupta, Abhinav and Tulsiani, Shubham},
	month = jun,
	year = {2022},
	pages = {3763--3772},
	file = {Alwala et al. - 2022 - Pretrain, Self-train, Distill A simple recipe for.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\9472PR6Y\\Alwala et al. - 2022 - Pretrain, Self-train, Distill A simple recipe for.pdf:application/pdf},
}

@misc{zhou_sparsefusion_2023,
	title = {{SparseFusion}: {Distilling} {View}-conditioned {Diffusion} for {3D} {Reconstruction}},
	shorttitle = {{SparseFusion}},
	url = {http://arxiv.org/abs/2212.00792},
	abstract = {We propose SparseFusion, a sparse view 3D reconstruction approach that unifies recent advances in neural rendering and probabilistic image generation. Existing approaches typically build on neural rendering with re-projected features but fail to generate unseen regions or handle uncertainty under large viewpoint changes. Alternate methods treat this as a (probabilistic) 2D synthesis task, and while they can generate plausible 2D images, they do not infer a consistent underlying 3D. However, we find that this trade-off between 3D consistency and probabilistic image generation does not need to exist. In fact, we show that geometric consistency and generative inference can be complementary in a mode-seeking behavior. By distilling a 3D consistent scene representation from a view-conditioned latent diffusion model, we are able to recover a plausible 3D representation whose renderings are both accurate and realistic. We evaluate our approach across 51 categories in the CO3D dataset and show that it outperforms existing methods, in both distortion and perception metrics, for sparse-view novel view synthesis.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhou, Zhizhuo and Tulsiani, Shubham},
	month = feb,
	year = {2023},
	note = {arXiv:2212.00792 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: project page: https://sparsefusion.github.io/ v2: typo corrected in table 3 v3: added ablation},
	file = {Zhou and Tulsiani - 2023 - SparseFusion Distilling View-conditioned Diffusio.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\3LJ96ZFH\\Zhou and Tulsiani - 2023 - SparseFusion Distilling View-conditioned Diffusio.pdf:application/pdf},
}

@techreport{jin_transfer_2021,
	type = {preprint},
	title = {Transfer learning framework for cell segmentation with incorporation of geometric features},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.02.28.433289},
	abstract = {With recent advances in multiplexed imaging and spatial transcriptomic and proteomic technologies, cell segmentation is becoming a crucial step in biomedical image analysis. In recent years, Fully Convolutional Networks (FCN) have achieved great success in nuclei segmentation in in vitro imaging. Nevertheless, it remains challenging to perform similar tasks on in situ tissue images with more cluttered cells of diverse shapes. To address this issue, we propose a novel transfer learning, cell segmentation framework incorporating shape-aware features in a deep learning model, with multi-level watershed and morphological post-processing steps. Our results show that incorporation of geometric features improves generalizability to segmenting cells in in situ tissue images, using solely in vitro images as training data.},
	language = {en},
	urldate = {2023-12-14},
	institution = {Systems Biology},
	author = {Jin, Yinuo and Toberoff, Alexandre and Azizi, Elham},
	month = mar,
	year = {2021},
	doi = {10.1101/2021.02.28.433289},
	file = {Jin et al. - 2021 - Transfer learning framework for cell segmentation .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\7W6MXCWK\\Jin et al. - 2021 - Transfer learning framework for cell segmentation .pdf:application/pdf},
}

@article{noauthor_cellstitch_nodate,
	title = {{CellStitch}: {3D} {Cellular} {Anisotropic} {Image} {Segmentation} via {Optimal} {Transport}},
	abstract = {Methods: To address these challenges, we formulate the problem of finding cell correspondence 23 across layers with a novel optimal transport (OT) approach. We propose CellStitch, a flexible 24 pipeline that segments cells from 3D images without requiring large amounts of 3D training 25 data. We further extend our method to interpolate internal slices from highly anisotropic cell 26 images to recover isotropic cell morphology. 27
Results: We evaluated the performance of CellStitch through eight 3D plant microscopic 28 datasets with diverse anisotropic levels and cell shapes. CellStitch substantially outperforms the 29 state-of-the art methods on anisotropic images, and achieves comparable segmentation quality 30 against competing methods in isotropic setting. We benchmarked and reported 3D segmenta31 tion results of all the methods with instance-level precision, recall and average precision (AP) 32 metrics. 33
Conclusion: The proposed OT-based 3D segmentation pipeline outperformed the existing 1},
	language = {en},
	file = {CellStitch 3D Cellular Anisotropic Image Segmenta.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\KXDWAZZ7\\CellStitch 3D Cellular Anisotropic Image Segmenta.pdf:application/pdf},
}

@inproceedings{liu_design_2022,
	address = {New Orleans LA USA},
	title = {Design {Guidelines} for {Prompt} {Engineering} {Text}-to-{Image} {Generative} {Models}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501825},
	doi = {10.1145/3491102.3501825},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Liu, Vivian and Chilton, Lydia B},
	month = apr,
	year = {2022},
	pages = {1--23},
	file = {Liu and Chilton - 2022 - Design Guidelines for Prompt Engineering Text-to-I.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\DBYHDY85\\Liu and Chilton - 2022 - Design Guidelines for Prompt Engineering Text-to-I.pdf:application/pdf},
}

@inproceedings{qiao_initial_2022,
	address = {Venice Italy},
	title = {Initial {Images}: {Using} {Image} {Prompts} to {Improve} {Subject} {Representation} in {Multimodal} {AI} {Generated} {Art}},
	isbn = {978-1-4503-9327-0},
	shorttitle = {Initial {Images}},
	url = {https://dl.acm.org/doi/10.1145/3527927.3532792},
	doi = {10.1145/3527927.3532792},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Creativity and {Cognition}},
	publisher = {ACM},
	author = {Qiao, Han and Liu, Vivian and Chilton, Lydia},
	month = jun,
	year = {2022},
	pages = {15--28},
	file = {Qiao et al. - 2022 - Initial Images Using Image Prompts to Improve Sub.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\2SM85EIN\\Qiao et al. - 2022 - Initial Images Using Image Prompts to Improve Sub.pdf:application/pdf},
}

@misc{epstein_oops_2019,
	title = {Oops! {Predicting} {Unintentional} {Action} in {Video}},
	url = {http://arxiv.org/abs/1911.11206},
	abstract = {From just a short glance at a video, we can often tell whether a person’s action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-thewild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and analyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pretraining. However, a signiﬁcant gap between machine and human performance remains.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
	month = nov,
	year = {2019},
	note = {arXiv:1911.11206 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 11 pages, 9 figures},
	file = {Epstein et al. - 2019 - Oops! Predicting Unintentional Action in Video.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\7FZJS7DF\\Epstein et al. - 2019 - Oops! Predicting Unintentional Action in Video.pdf:application/pdf},
}

@misc{liu_zero-1--3_2023,
	title = {Zero-1-to-3: {Zero}-shot {One} {Image} to {3D} {Object}},
	shorttitle = {Zero-1-to-3},
	url = {http://arxiv.org/abs/2303.11328},
	abstract = {We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11328 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
	annote = {Comment: Website: https://zero123.cs.columbia.edu/},
	file = {Liu et al. - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\DL6KNTYI\\Liu et al. - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf:application/pdf},
}

@inproceedings{liu_humans_2023,
	address = {Vancouver, BC, Canada},
	title = {Humans as {Light} {Bulbs}: {3D} {Human} {Reconstruction} from {Thermal} {Reflection}},
	isbn = {9798350301298},
	shorttitle = {Humans as {Light} {Bulbs}},
	url = {https://ieeexplore.ieee.org/document/10204641/},
	doi = {10.1109/CVPR52729.2023.01206},
	abstract = {The relatively hot temperature of the human body causes people to turn into long-wave infrared light sources. Since this emitted light has a larger wavelength than visible light, many surfaces in typical scenes act as infrared mirrors with strong specular reﬂections. We exploit the thermal reﬂections of a person onto objects in order to locate their position and reconstruct their pose, even if they are not visible to a normal camera. We propose an analysis-by-synthesis framework that jointly models the objects, people, and their thermal reﬂections, which combines generative models with differentiable rendering of reﬂections. Quantitative and qualitative experiments show our approach works in highly challenging cases, such as with curved mirrors or when the person is completely unseen by a normal camera.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Ruoshi and Vondrick, Carl},
	month = jun,
	year = {2023},
	pages = {12531--12542},
	file = {Liu and Vondrick - 2023 - Humans as Light Bulbs 3D Human Reconstruction fro.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\ERPPZ4UX\\Liu and Vondrick - 2023 - Humans as Light Bulbs 3D Human Reconstruction fro.pdf:application/pdf},
}

@incollection{malawski_3d_2020,
	address = {Cham},
	title = {{3D} {Coded} {SUMMA}: {Communication}-{Efficient} and {Robust} {Parallel} {Matrix} {Multiplication}},
	volume = {12247},
	isbn = {978-3-030-57674-5 978-3-030-57675-2},
	shorttitle = {{3D} {Coded} {SUMMA}},
	url = {https://link.springer.com/10.1007/978-3-030-57675-2_25},
	abstract = {In this paper, we propose a novel fault-tolerant parallel matrix multiplication algorithm called 3D Coded SUMMA that achieves higher failure-tolerance than replication-based schemes for the same amount of redundancy. This work bridges the gap between recent developments in coded computing and fault-tolerance in high-performance computing (HPC). The core idea of coded computing is the same as algorithm-based fault-tolerance (ABFT), which is weaving redundancy in the computation using error-correcting codes. In particular, we show that MatDot codes, an innovative code construction for parallel matrix multiplications, can be integrated into three-dimensional SUMMA (Scalable Universal Matrix Multiplication Algorithm [30]) in a communicationavoiding manner. To tolerate any two node failures, the proposed 3D Coded SUMMA requires ∼50\% less redundancy than replication, while the overhead in execution time is only about 5–10\%.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Euro-{Par} 2020: {Parallel} {Processing}},
	publisher = {Springer International Publishing},
	author = {Jeong, Haewon and Yang, Yaoqing and Gupta, Vipul and Engelmann, Christian and Low, Tze Meng and Cadambe, Viveck and Ramchandran, Kannan and Grover, Pulkit},
	editor = {Malawski, Maciej and Rzadca, Krzysztof},
	year = {2020},
	doi = {10.1007/978-3-030-57675-2_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {392--407},
	file = {Jeong et al. - 2020 - 3D Coded SUMMA Communication-Efficient and Robust.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\3AAEZBP3\\Jeong et al. - 2020 - 3D Coded SUMMA Communication-Efficient and Robust.pdf:application/pdf},
}

@article{chen_deep_2020,
	title = {Deep {Unsupervised} {Learning} of {3D} {Point} {Clouds} via {Graph} {Topology} {Inference} and {Filtering}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	url = {http://arxiv.org/abs/1905.04571},
	doi = {10.1109/TIP.2019.2957935},
	abstract = {We propose a deep autoencoder with graph topology inference and ﬁltering to achieve compact representations of unorganized 3D point clouds in an unsupervised manner. Many previous works discretize 3D points to voxels and then use latticebased methods to process and learn 3D spatial information; however, this leads to inevitable discretization errors. In this work, we try to handle raw 3D points without such compromise. The proposed networks follow the autoencoder framework with a focus on designing the decoder. The encoder of the proposed networks adopts similar architectures as in PointNet, which is a well-acknowledged method for supervised learning of 3D point clouds. The decoder of the proposed networks involves three novel modules: the folding module, the graph-topologyinference module, and the graph-ﬁltering module. The folding module folds a canonical 2D lattice to the underlying surface of a 3D point cloud, achieving coarse reconstruction; the graphtopology-inference module learns a graph topology to represent pairwise relationships between 3D points, pushing the latent code to preserve both coordinates and pairwise relationships of points in 3D point clouds; and the graph-ﬁltering module couples the above two modules, reﬁning the coarse reconstruction through a learnt graph topology to obtain the ﬁnal reconstruction. The proposed decoder leverages a learnable graph topology to push the codeword to preserve representative features and further improve the unsupervised-learning performance. We further provide theoretical analyses of the proposed architecture. We provide an upper bound for the reconstruction loss and further show the superiority of graph smoothness over spatial smoothness as a prior to model 3D point clouds. In the experiments, we validate the proposed networks in three tasks, including 3D point cloud reconstruction, visualization, and transfer classiﬁcation. The experimental results show that (1) the proposed networks outperform the state-of-the-art methods in various tasks, including reconstruction and transfer classiﬁcation; (2) a graph topology can be inferred as auxiliary information without speciﬁc supervision on graph topology inference; (3) graph ﬁltering reﬁnes the reconstruction, leading to better performances; and (4) designing a powerful decoder could improve the unsupervisedlearning performance, just like a powerful encoder.},
	language = {en},
	urldate = {2023-12-14},
	journal = {IEEE Transactions on Image Processing},
	author = {Chen, Siheng and Duan, Chaojing and Yang, Yaoqing and Li, Duanshun and Feng, Chen and Tian, Dong},
	year = {2020},
	note = {arXiv:1905.04571 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3183--3198},
	annote = {Comment: To appear in IEEE Transactions on Image Processing},
	file = {Chen et al. - 2020 - Deep Unsupervised Learning of 3D Point Clouds via .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NFJBS7PE\\Chen et al. - 2020 - Deep Unsupervised Learning of 3D Point Clouds via .pdf:application/pdf},
}

@misc{trivedi_augmentations_2022,
	title = {Augmentations in {Graph} {Contrastive} {Learning}: {Current} {Methodological} {Flaws} \& {Towards} {Better} {Practices}},
	shorttitle = {Augmentations in {Graph} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2111.03220},
	abstract = {Unsupervised graph representation learning is critical to a wide range of applications where labels may be scarce or expensive to procure. Contrastive learning (CL) is an increasingly popular paradigm for such settings and the state-of-the-art in unsupervised visual representation learning. Recent work attributes the success of visual CL to use of task-relevant augmentations and large, diverse datasets. Interestingly, graph CL frameworks report strong performance despite using orders of magnitude smaller datasets and employing domain-agnostic graph augmentations (DAGAs). Motivated by this discrepancy, we probe the quality of representations learnt by popular graph CL frameworks using DAGAs. We find that DAGAs can destroy task-relevant information and harm the model’s ability to learn discriminative representations. On small benchmark datasets, we show the inductive bias of graph neural networks can significantly compensate for this weak discriminability. Based on our findings, we propose several sanity checks that enable practitioners to quickly assess the quality of their model’s learned representations. We further propose a broad strategy for designing task-aware augmentations that are amenable to graph CL and demonstrate its efficacy on two large-scale, complex graph applications. For example, in graph-based document classification, we show task-relevant augmentations improve accuracy up to 20\%.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Trivedi, Puja and Lubana, Ekdeep Singh and Yan, Yujun and Yang, Yaoqing and Koutra, Danai},
	month = mar,
	year = {2022},
	note = {arXiv:2111.03220 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 8 pages, 4 figures, Accepted WebConf 2022},
	file = {Trivedi et al. - 2022 - Augmentations in Graph Contrastive Learning Curre.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\XDXVDSQL\\Trivedi et al. - 2022 - Augmentations in Graph Contrastive Learning Curre.pdf:application/pdf},
}

@article{zhou_temperature_nodate,
	title = {Temperature {Balancing}, {Layer}-wise {Weight} {Analysis}, and {Neural} {Network} {Training}},
	abstract = {Regularization in modern machine learning is crucial, and it can take various forms in algorithmic design: training set, model family, error function, regularization terms, and optimizations. In particular, the learning rate, which can be interpreted as a temperature-like parameter within the statistical mechanics of learning, plays a crucial role in neural network training. Indeed, many widely adopted training strategies basically just define the decay of the learning rate over time. This process can be interpreted as decreasing a temperature, using either a global learning rate (for the entire model) or a learning rate that varies for each parameter. This paper proposes TempBalance, a straightforward yet effective layer-wise learning rate method. TempBalance is based on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which characterizes the implicit self-regularization of different layers in trained models. We demonstrate the efficacy of using HT-SR-motivated metrics to guide the scheduling and balancing of temperature across all network layers during model training, resulting in improved performance during testing. We implement TempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using ResNets, VGGs and WideResNets with various depths and widths. Our results show that TempBalance significantly outperforms ordinary SGD and carefully-tuned spectral norm regularization. We also show that TempBalance outperforms a number of state-of-the-art optimizers and learning rate schedulers.},
	language = {en},
	author = {Zhou, Yefan and Pang, Tianyu and Liu, Keqin and Martin, Charles H and Mahoney, Michael W and Yang, Yaoqing},
	file = {Zhou et al. - Temperature Balancing, Layer-wise Weight Analysis,.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\4FABUHC6\\Zhou et al. - Temperature Balancing, Layer-wise Weight Analysis,.pdf:application/pdf},
}

@article{yap_deep_2021,
	title = {Deep learning in diabetic foot ulcers detection: {A} comprehensive evaluation},
	volume = {135},
	issn = {00104825},
	shorttitle = {Deep learning in diabetic foot ulcers detection},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521003905},
	doi = {10.1016/j.compbiomed.2021.104596},
	abstract = {There has been a substantial amount of research involving computer methods and technology for the detection and recognition of diabetic foot ulcers (DFUs), but there is a lack of systematic comparisons of state-of-the-art deep learning object detection frameworks applied to this problem. DFUC2020 provided participants with a comprehensive dataset consisting of 2,000 images for training and 2,000 images for testing. This paper sum­ marizes the results of DFUC2020 by comparing the deep learning-based algorithms proposed by the winning teams: Faster R–CNN, three variants of Faster R–CNN and an ensemble method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For each deep learning method, we provide a detailed description of model architecture, parameter settings for training and additional stages including pre-processing, data augmentation and post-processing. We provide a comprehensive evaluation for each method. All the methods required a data augmentation stage to increase the number of images available for training and a post-processing stage to remove false positives. The best performance was obtained from Deformable Convolution, a variant of Faster R–CNN, with a mean average precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we demonstrate that the ensemble method based on different deep learning methods can enhance the F1-Score but not the mAP.},
	language = {en},
	urldate = {2023-12-14},
	journal = {Computers in Biology and Medicine},
	author = {Yap, Moi Hoon and Hachiuma, Ryo and Alavi, Azadeh and Brüngel, Raphael and Cassidy, Bill and Goyal, Manu and Zhu, Hongtao and Rückert, Johannes and Olshansky, Moshe and Huang, Xiao and Saito, Hideo and Hassanpour, Saeed and Friedrich, Christoph M. and Ascher, David B. and Song, Anping and Kajita, Hiroki and Gillespie, David and Reeves, Neil D. and Pappachan, Joseph M. and O'Shea, Claire and Frank, Eibe},
	month = aug,
	year = {2021},
	pages = {104596},
	file = {Yap et al. - 2021 - Deep learning in diabetic foot ulcers detection A.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\I4T7CVGN\\Yap et al. - 2021 - Deep learning in diabetic foot ulcers detection A.pdf:application/pdf},
}

@article{dipalma_resolution-based_2021,
	title = {Resolution-based distillation for efficient histology image classification},
	volume = {119},
	issn = {09333657},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365721001299},
	doi = {10.1016/j.artmed.2021.102136},
	abstract = {Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classi­ fication. The proposed approach is robust when used with images that have reduced input resolution, and it can be trained effectively with limited labeled data. Moreover, our approach operates at either the tissue- or slidelevel, removing the need for laborious patch-level labeling. Our method uses knowledge distillation to trans­ fer knowledge from a teacher model pre-trained at high resolution to a student model trained on the same images at a considerably lower resolution. Also, to address the lack of large-scale labeled histology image datasets, we perform the knowledge distillation in a self-supervised fashion. We evaluate our approach on three distinct histology image datasets associated with celiac disease, lung adenocarcinoma, and renal cell carcinoma. Our results on these datasets demonstrate that a combination of knowledge distillation and self-supervision allows the student model to approach and, in some cases, surpass the teacher model's classification accuracy while being much more computationally efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential for this method to scale further with additional unlabeled data. Our model outperforms the high-resolution teacher model for celiac disease in accuracy, F1-score, precision, and recall while requiring 4 times fewer computations. For lung adenocarcinoma, our results at 1.25× magnification are within 1.5\% of the results for the teacher model at 10× magnification, with a reduction in computational cost by a factor of 64. Our model on renal cell carcinoma at 1.25× magni­ fication performs within 1\% of the teacher model at 5× magnification while requiring 16 times fewer compu­ tations. Furthermore, our celiac disease outcomes benefit from additional performance scaling with the use of more unlabeled data. In the case of 0.625× magnification, using unlabeled data improves accuracy by 4\% over the tissue-level baseline. Therefore, our approach can improve the feasibility of deep learning solutions for digital pathology on standard computational hardware and infrastructures.},
	language = {en},
	urldate = {2023-12-14},
	journal = {Artificial Intelligence in Medicine},
	author = {DiPalma, Joseph and Suriawinata, Arief A. and Tafe, Laura J. and Torresani, Lorenzo and Hassanpour, Saeed},
	month = sep,
	year = {2021},
	pages = {102136},
	file = {DiPalma et al. - 2021 - Resolution-based distillation for efficient histol.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\V6FZSQPT\\DiPalma et al. - 2021 - Resolution-based distillation for efficient histol.pdf:application/pdf},
}

@article{jiang_multi-ontology_2020,
	title = {Multi-{Ontology} {Refined} {Embeddings} ({MORE}): {A} hybrid multi-ontology and corpus-based semantic representation model for biomedical concepts},
	volume = {111},
	issn = {15320464},
	shorttitle = {Multi-{Ontology} {Refined} {Embeddings} ({MORE})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046420302094},
	doi = {10.1016/j.jbi.2020.103581},
	abstract = {Objective: Currently, a major limitation for natural language processing (NLP) analyses in clinical applications is that concepts are not effectively referenced in various forms across different texts. This paper introduces MultiOntology Refined Embeddings (MORE), a novel hybrid framework that incorporates domain knowledge from multiple ontologies into a distributional semantic model, learned from a corpus of clinical text. Materials and Methods: We use the RadCore and MIMIC-III free-text datasets for the corpus-based component of MORE. For the ontology-based part, we use the Medical Subject Headings (MeSH) ontology and three state-ofthe-art ontology-based similarity measures. In our approach, we propose a new learning objective, modified from the sigmoid cross-entropy objective function.
Results and Discussion: We used two established datasets of semantic similarities among biomedical concept pairs to evaluate the quality of the generated word embeddings. On the first dataset with 29 concept pairs, with similarity scores established by physicians and medical coders, MORE’s similarity scores have the highest combined correlation (0.633), which is 5.0\% higher than that of the baseline model, and 12.4\% higher than that of the best ontology-based similarity measure. On the second dataset with 449 concept pairs, MORE’s similarity scores have a correlation of 0.481, based on the average of four medical residents’ similarity ratings, and that outperforms the skip-gram model by 8.1\%, and the best ontology measure by 6.9\%. Furthermore, MORE out­ performs three pre-trained transformer-based word embedding models (i.e., BERT, ClinicalBERT, and BioBERT) on both datasets.
Conclusion: MORE incorporates knowledge from several biomedical ontologies into an existing corpus-based distributional semantics model, improving both the accuracy of the learned word embeddings and the extensi­ bility of the model to a broader range of biomedical concepts. MORE allows for more accurate clustering of concepts across a wide range of applications, such as analyzing patient health records to identify subjects with similar pathologies, or integrating heterogeneous clinical data to improve interoperability between hospitals.},
	language = {en},
	urldate = {2023-12-14},
	journal = {Journal of Biomedical Informatics},
	author = {Jiang, Steven and Wu, Weiyi and Tomita, Naofumi and Ganoe, Craig and Hassanpour, Saeed},
	month = nov,
	year = {2020},
	pages = {103581},
	file = {Jiang et al. - 2020 - Multi-Ontology Refined Embeddings (MORE) A hybrid.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\X7NY7XCB\\Jiang et al. - 2020 - Multi-Ontology Refined Embeddings (MORE) A hybrid.pdf:application/pdf},
}

@article{dipalma_histoperm_2023,
	title = {{HistoPerm}: {A} permutation-based view generation approach for improving histopathologic feature representation learning},
	volume = {14},
	issn = {21533539},
	shorttitle = {{HistoPerm}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2153353923001347},
	doi = {10.1016/j.jpi.2023.100320},
	abstract = {Deep learning has been effective for histology image analysis in digital pathology. However, many current deep learning approaches require large, strongly- or weakly labeled images and regions of interest, which can be time-consuming and resource-intensive to obtain. To address this challenge, we present HistoPerm, a view generation method for representation learning using joint embedding architectures that enhances representation learning for histology images. HistoPerm permutes augmented views of patches extracted from whole-slide histology images to improve classiﬁcation performance. We evaluated the effectiveness of HistoPerm on 2 histology image datasets for Celiac disease and Renal Cell Carcinoma, using 3 widely used joint embedding architecture-based representation learning methods: BYOL, SimCLR, and VICReg. Our results show that HistoPerm consistently improves patch- and slide-level classiﬁcation performance in terms of accuracy, F1-score, and AUC. Speciﬁcally, for patch-level classiﬁcation accuracy on the Celiac disease dataset, HistoPerm boosts BYOL and VICReg by 8\% and SimCLR by 3\%. On the Renal Cell Carcinoma dataset, patch-level classiﬁcation accuracy is increased by 2\% for BYOL and VICReg, and by 1\% for SimCLR. In addition, on the Celiac disease dataset, models with HistoPerm outperform the fully supervised baseline model by 6\%, 5\%, and 2\% for BYOL, SimCLR, and VICReg, respectively. For the Renal Cell Carcinoma dataset, HistoPerm lowers the classiﬁcation accuracy gap for the models up to 10\% relative to the fully supervised baseline. These ﬁndings suggest that HistoPerm can be a valuable tool for improving representation learning of histopathology features when access to labeled data is limited and can lead to whole-slide classiﬁcation results that are comparable to or superior to fully supervised methods.},
	language = {en},
	urldate = {2023-12-14},
	journal = {Journal of Pathology Informatics},
	author = {DiPalma, Joseph and Torresani, Lorenzo and Hassanpour, Saeed},
	year = {2023},
	pages = {100320},
	file = {DiPalma et al. - 2023 - HistoPerm A permutation-based view generation app.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\VLPRBVBV\\DiPalma et al. - 2023 - HistoPerm A permutation-based view generation app.pdf:application/pdf},
}

@article{tomita_automatic_2020,
	title = {Automatic post-stroke lesion segmentation on {MR} images using {3D} residual convolutional neural network},
	volume = {27},
	issn = {22131582},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158220301133},
	doi = {10.1016/j.nicl.2020.102276},
	abstract = {In this paper, we demonstrate the feasibility and performance of deep residual neural networks for volumetric segmentation of irreversibly damaged brain tissue lesions on T1-weighted MRI scans for chronic stroke patients. A total of 239 T1-weighted MRI scans of chronic ischemic stroke patients from a public dataset were retrospectively analyzed by 3D deep convolutional segmentation models with residual learning, using a novel zoomin\&out strategy. Dice similarity coeﬃcient (DSC), average symmetric surface distance (ASSD), and Hausdorﬀ distance (HD) of the identiﬁed lesions were measured by using manual tracing of lesions as the reference standard. Bootstrapping was employed for all metrics to estimate 95\% conﬁdence intervals. The models were assessed on a test set of 31 scans. The average DSC was 0.64 (0.51–0.76) with a median of 0.78. ASSD and HD were 3.6 mm (1.7–6.2 mm) and 20.4 mm (10.0–33.3 mm), respectively. The latest deep learning architecture and techniques were applied with 3D segmentation on MRI scans and demonstrated eﬀectiveness for volumetric segmentation of chronic ischemic stroke lesions.},
	language = {en},
	urldate = {2023-12-14},
	journal = {NeuroImage: Clinical},
	author = {Tomita, Naofumi and Jiang, Steven and Maeder, Matthew E. and Hassanpour, Saeed},
	year = {2020},
	pages = {102276},
	file = {Tomita et al. - 2020 - Automatic post-stroke lesion segmentation on MR im.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\WJV397LN\\Tomita et al. - 2020 - Automatic post-stroke lesion segmentation on MR im.pdf:application/pdf},
}

@incollection{tucker_petri_2021,
	address = {Cham},
	title = {A {Petri} {Dish} for {Histopathology} {Image} {Analysis}},
	volume = {12721},
	isbn = {978-3-030-77210-9 978-3-030-77211-6},
	url = {https://link.springer.com/10.1007/978-3-030-77211-6_2},
	abstract = {With the rise of deep learning, there has been increased interest in using neural networks for histopathology image analysis, a ﬁeld that investigates the properties of biopsy or resected specimens traditionally manually examined under a microscope by pathologists. However, challenges such as limited data, costly annotation, and processing high-resolution and variable-size images make it difﬁcult to quickly iterate over model designs.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Wei, Jerry and Suriawinata, Arief and Ren, Bing and Liu, Xiaoying and Lisovsky, Mikhail and Vaickus, Louis and Brown, Charles and Baker, Michael and Tomita, Naofumi and Torresani, Lorenzo and Wei, Jason and Hassanpour, Saeed},
	editor = {Tucker, Allan and Henriques Abreu, Pedro and Cardoso, Jaime and Pereira Rodrigues, Pedro and Riaño, David},
	year = {2021},
	doi = {10.1007/978-3-030-77211-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {11--24},
	file = {Wei et al. - 2021 - A Petri Dish for Histopathology Image Analysis.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\JBUU8RXN\\Wei et al. - 2021 - A Petri Dish for Histopathology Image Analysis.pdf:application/pdf},
}

@misc{goyal_multi-class_2020,
	title = {Multi-class {Semantic} {Segmentation} of {Skin} {Lesions} via {Fully} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1711.10449},
	abstract = {Melanoma is clinically diﬃcult to distinguish from common benign skin lesions, particularly melanocytic naevus and seborrhoeic keratosis. The dermoscopic appearance of these lesions has huge intra-class variations and high inter-class visual similarities. Most current research is focusing on single-class segmentation irrespective of classes of skin lesions. In this work, we evaluate the performance of deep learning on multi-class segmentation of ISIC-2017 challenge dataset, which consists of 2,750 dermoscopic images. We propose an end-to-end solution using fully convolutional networks (FCNs) for multi-class semantic segmentation to automatically segment the melanoma, seborrhoeic keratosis and naevus. To improve the performance of FCNs, transfer learning and a hybrid loss function are used. We evaluate the performance of the deep learning segmentation methods for multi-class segmentation and lesion diagnosis (with post-processing method) on the testing set of the ISIC-2017 challenge dataset. The results showed that the two-tier level transfer learning FCN-8s achieved the overall best result with Dice score of 78.5\% in a naevus category, 65.3\% in melanoma, and 55.7\% in seborrhoeic keratosis in multiclass segmentation and Accuracy of 84.62\% for recognition of melanoma in lesion diagnosis.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Goyal, Manu and Yap, Moi Hoon and Hassanpour, Saeed},
	month = mar,
	year = {2020},
	note = {arXiv:1711.10449 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Comp2clinic workshop at Biostec 2020},
	file = {Goyal et al. - 2020 - Multi-class Semantic Segmentation of Skin Lesions .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\S7BF9ZXP\\Goyal et al. - 2020 - Multi-class Semantic Segmentation of Skin Lesions .pdf:application/pdf},
}

@inproceedings{ma_improving_2023,
	address = {Toronto, Canada},
	title = {Improving {Syntactic} {Probing} {Correctness} and {Robustness} with {Control} {Tasks}},
	url = {https://aclanthology.org/2023.acl-short.35},
	doi = {10.18653/v1/2023.acl-short.35},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Weicheng and Wang, Brian and Zhang, Hefan and Wang, Lili and Coto-Solano, Rolando and Hassanpour, Saeed and Vosoughi, Soroush},
	year = {2023},
	pages = {402--415},
	file = {Ma et al. - 2023 - Improving Syntactic Probing Correctness and Robust.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\2DNBZV5M\\Ma et al. - 2023 - Improving Syntactic Probing Correctness and Robust.pdf:application/pdf},
}

@misc{wei_calibrating_2022,
	title = {Calibrating {Histopathology} {Image} {Classifiers} using {Label} {Smoothing}},
	url = {http://arxiv.org/abs/2201.11866},
	abstract = {The classiﬁcation of histopathology images fundamentally differs from traditional image classiﬁcation tasks because histopathology images naturally exhibit a range of diagnostic features, resulting in a diverse range of annotator agreement levels. However, examples with high annotator disagreement are often either assigned the majority label or discarded entirely when training histopathology image classiﬁers. This widespread practice often yields classiﬁers that do not account for example difﬁculty and exhibit poor model calibration. In this paper, we ask: can we improve model calibration by endowing histopathology image classiﬁers with inductive biases about example difﬁculty? We propose several label smoothing methods that utilize per-image annotator agreement. Though our methods are simple, we ﬁnd that they substantially improve model calibration, while maintaining (or even improving) accuracy. For colorectal polyp classiﬁcation, a common yet challenging task in gastrointestinal pathology, we ﬁnd that our proposed agreement-aware label smoothing methods reduce calibration error by almost 70\%. Moreover, we ﬁnd that using model conﬁdence as a proxy for annotator agreement also improves calibration and accuracy, suggesting that datasets without multiple annotators can still beneﬁt from our proposed label smoothing methods via our proposed conﬁdence-aware label smoothing methods.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wei, Jerry and Torresani, Lorenzo and Wei, Jason and Hassanpour, Saeed},
	month = jan,
	year = {2022},
	note = {arXiv:2201.11866 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Wei et al. - 2022 - Calibrating Histopathology Image Classifiers using.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\CE8H6PF9\\Wei et al. - 2022 - Calibrating Histopathology Image Classifiers using.pdf:application/pdf},
}

@inproceedings{xie_towards_2022,
	address = {Montreal, QC, Canada},
	title = {Towards {Interpretable} {Deep} {Reinforcement} {Learning} {Models} via {Inverse} {Reinforcement} {Learning}},
	isbn = {978-1-66549-062-7},
	url = {https://ieeexplore.ieee.org/document/9956245/},
	doi = {10.1109/ICPR56361.2022.9956245},
	abstract = {Artificial Intelligence, particularly through recent advancements in deep learning (DL), has achieved exceptional performances in many tasks in fields such as natural language processing and computer vision. For certain high-stake domains, in addition to desirable performance metrics, a high level of interpretability is often required in order for AI to be reliably utilized. Unfortunately, the black box nature of DL models prevents researchers from providing explicative descriptions for a DL model’s reasoning process and decisions. In this work, we propose a novel framework utilizing Adversarial Inverse Reinforcement Learning that can provide global explanations for decisions made by a Reinforcement Learning model and capture intuitive tendencies that the model follows by summarizing the model’s decision-making process.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Xie, Yuansheng and Vosoughi, Soroush and Hassanpour, Saeed},
	month = aug,
	year = {2022},
	pages = {5067--5074},
	file = {Xie et al. - 2022 - Towards Interpretable Deep Reinforcement Learning .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\58VQC4ZL\\Xie et al. - 2022 - Towards Interpretable Deep Reinforcement Learning .pdf:application/pdf},
}

@misc{zhou_dataset-dispersion_2021,
	title = {A {Dataset}-{Dispersion} {Perspective} on {Reconstruction} {Versus} {Recognition} in {Single}-{View} {3D} {Reconstruction} {Networks}},
	url = {http://arxiv.org/abs/2111.15158},
	abstract = {Neural networks (NN) for single-view 3D reconstruction (SVR) have gained in popularity. Recent work points out that for SVR, most cutting-edge NNs have limited performance on reconstructing unseen objects because they rely primarily on recognition (i.e., classification-based methods) rather than shape reconstruction. To understand this issue in depth, we provide a systematic study on when and why NNs prefer recognition to reconstruction and vice versa. Our finding shows that a leading factor in determining recognition versus reconstruction is how dispersed the training data is. Thus, we introduce the dispersion score, a new data-driven metric, to quantify this leading factor and study its effect on NNs. We hypothesize that NNs are biased toward recognition when training images are more dispersed and training shapes are less dispersed. Our hypothesis is supported and the dispersion score is proved effective through our experiments on synthetic and benchmark datasets. We show that the proposed metric is a principal way to analyze reconstruction quality and provides novel information in addition to the conventional reconstruction score.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhou, Yefan and Shen, Yiru and Yan, Yujun and Feng, Chen and Yang, Yaoqing},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15158 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to 3DV 2021},
	file = {Zhou et al. - 2021 - A Dataset-Dispersion Perspective on Reconstruction.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\PJSYEJ7R\\Zhou et al. - 2021 - A Dataset-Dispersion Perspective on Reconstruction.pdf:application/pdf},
}

@misc{yan_size_2023,
	title = {Size {Generalization} of {Graph} {Neural} {Networks} on {Biological} {Data}: {Insights} and {Practices} from the {Spectral} {Perspective}},
	shorttitle = {Size {Generalization} of {Graph} {Neural} {Networks} on {Biological} {Data}},
	url = {http://arxiv.org/abs/2305.15611},
	abstract = {We investigate size-induced distribution shifts in graphs and assess their impact on the ability of graph neural networks (GNNs) to generalize to larger graphs relative to the training data. Existing literature presents conflicting conclusions on GNNs’ size generalizability, primarily due to disparities in application domains and underlying assumptions concerning size-induced distribution shifts. Motivated by this, we take a data-driven approach: we focus on real biological datasets and seek to characterize the types of size-induced distribution shifts. Diverging from prior approaches, we adopt a spectral perspective and identify that spectrum differences induced by size are related to differences in subgraph patterns (e.g., average cycle lengths). We further find that common GNNs cannot capture these subgraph patterns, resulting in performance decline when testing on larger graphs. Based on these spectral insights, we introduce and compare three model-agnostic strategies aimed at making GNNs aware of important subgraph patterns to enhance their size generalizability: self-supervision, augmentation, and size-insensitive attention. Our empirical results reveal that all strategies enhance GNNs’ size generalizability, with simple size-insensitive attention surprisingly emerging as the most effective method. Notably, this strategy substantially enhances graph classification performance on large test graphs, which are 2-10 times larger than the training graphs, resulting in an improvement in F1 scores by up to 8\%.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Yan, Yujun and Li, Gaotang and koutra, Danai},
	month = sep,
	year = {2023},
	note = {arXiv:2305.15611 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 17 pages, including appendix},
	file = {Yan et al. - 2023 - Size Generalization of Graph Neural Networks on Bi.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\DI23AQNZ\\Yan et al. - 2023 - Size Generalization of Graph Neural Networks on Bi.pdf:application/pdf},
}

@inproceedings{li_interpretable_2023,
	title = {Interpretable {Sparsification} of {Brain} {Graphs}: {Better} {Practices} and {Effective} {Designs} for {Graph} {Neural} {Networks}},
	shorttitle = {Interpretable {Sparsification} of {Brain} {Graphs}},
	url = {http://arxiv.org/abs/2306.14375},
	doi = {10.1145/3580305.3599394},
	abstract = {Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications involving graph classification. However, dense brain graphs pose computational challenges including high runtime and memory usage and limited interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. While prior works remove noisy edges based on explainability or task-irrelevant properties, their effectiveness in enhancing performance with sparsified graphs is not guaranteed. Moreover, existing approaches often overlook collective edge removal across multiple graphs. To address these issues, we introduce an iterative framework to analyze different sparsification models. Our findings are as follows: (i) methods prioritizing interpretability may not be suitable for graph sparsification as they can degrade GNNs’ performance in graph classification tasks; (ii) simultaneously learning edge selection with GNN training is more beneficial than post-training; (iii) a shared edge selection across graphs outperforms separate selection for each graph; and (iv) task-relevant gradient information aids in edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which enhances graph classification performance by up to 5.1\% with 55.0\% fewer edges. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Li, Gaotang and Duda, Marlena and Zhang, Xiang and Koutra, Danai and Yan, Yujun},
	month = aug,
	year = {2023},
	note = {arXiv:2306.14375 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {1223--1234},
	annote = {Comment: To appear in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23)},
	file = {Li et al. - 2023 - Interpretable Sparsification of Brain Graphs Bett.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\YNX7VY8Y\\Li et al. - 2023 - Interpretable Sparsification of Brain Graphs Bett.pdf:application/pdf},
}

@article{zhu_heterophily_nodate,
	title = {Heterophily and {Graph} {Neural} {Networks}: {Past}, {Present} and {Future}},
	abstract = {Recently, there has been interest in understanding the performance of Graph Neural Networks (GNNs) on input graphs exhibiting heterophily, or the tendency for nodes of different classes to connect. Initial findings showed that many standard GNN models struggled on certain benchmark datasets exhibiting high heterophily, prompting research into existing and novel GNN designs that improved learning in these contexts. However, further analyses revealed that certain highly heterophilous settings did not challenge GNNs without these specialized designs, raising questions about the true factors causing performance degradation. In this work, we first review various GNN designs proposed for handling graphs with heterophily, and examine their connections to other GNN research objectives such as robustness, fairness, and oversmoothing avoidance. Next, we conduct an empirical study to investigate the specific heterophilous graph conditions under which GNNs can and cannot perform effectively. Our analysis reveals that although high heterophily does not universally impede conventional GNNs, unique challenges in heterophilous graphs, particularly the intertwined effects with low-degree nodes and complex compatibility patterns, warrant GNN designs specifically tailored to heterophily. In conclusion, we discuss future research directions aimed at advancing the understanding of the impact of heterophily on GNNs across a broader range of contexts.},
	language = {en},
	author = {Zhu, Jiong and Yan, Yujun and Heimann, Mark and Zhao, Lingxiao and Akoglu, Leman and Koutra, Danai},
	file = {Zhu et al. - Heterophily and Graph Neural Networks Past, Prese.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\T3NFZ9EE\\Zhu et al. - Heterophily and Graph Neural Networks Past, Prese.pdf:application/pdf},
}

@article{zhang_single_nodate,
	title = {Single {Depth}-image {3D} {Reflection} {Symmetry} and {Shape} {Prediction}},
	abstract = {In this paper, we present Iterative Symmetry Completion Network (ISCNet), a single depth-image shape completion method that exploits reflective symmetry cues to obtain more detailed shapes. The efficacy of single depth-image shape completion methods is often sensitive to the accuracy of the symmetry plane. ISCNet therefore jointly estimates the symmetry plane and shape completion iteratively; more complete shapes contribute to more robust symmetry plane estimates and vice versa. Furthermore, our shape completion method operates in the image domain, enabling more efficient high-resolution, detailed geometry reconstruction. We perform the shape completion from pairs of viewpoints, reflected across the symmetry plane, predicted by a reinforcement learning agent to improve robustness and to simultaneously explicitly leverage symmetry. We demonstrate the effectiveness of ISCNet on a variety of object categories on both synthetic and real-scanned datasets.},
	language = {en},
	author = {Zhang, Zhaoxuan and Dong, Bo and Li, Tong and Heide, Felix and Peers, Pieter and Yin, Baocai and Yang, Xin},
	file = {Zhang et al. - Single Depth-image 3D Reflection Symmetry and Shap.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\6628GXPJ\\Zhang et al. - Single Depth-image 3D Reflection Symmetry and Shap.pdf:application/pdf},
}

@article{qiao_multi-view_nodate,
	title = {Multi-view {Spectral} {Polarization} {Propagation} for {Video} {Glass} {Segmentation}},
	abstract = {In this paper, we present the first polarization-guided video glass segmentation propagation solution (PGVS-Net) that can robustly and coherently propagate glass segmentation in RGB-P video sequences. By leveraging spatiotemporal polarization and color information, our method combines multi-view polarization cues and thus can alleviate the view dependence of single-input intensity variations on glass objects. We demonstrate that our model can outperform glass segmentation on RGB-only video sequences as well as produce more robust segmentation than per-frame RGB-P single-image segmentation methods. To train and validate PGVS-Net, we introduce a novel RGB-P Glass Video dataset (PGV-117) containing 117 video sequences of scenes captured with different types of camera paths, lighting conditions, dynamics, and glass types.},
	language = {en},
	author = {Qiao, Yu and Dong, Bo and Jin, Ao and Fu, Yu and Baek, Seung-Hwan and Heide, Felix and Peers, Pieter and Wei, Xiaopeng and Yang, Xin},
	file = {Qiao et al. - Multi-view Spectral Polarization Propagation for V.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\8RDQ6MLL\\Qiao et al. - Multi-view Spectral Polarization Propagation for V.pdf:application/pdf},
}

@misc{wu_vision-language_2023,
	title = {Vision-{Language} {Dataset} {Distillation}},
	url = {http://arxiv.org/abs/2308.07545},
	abstract = {Dataset distillation methods promise to reduce large-scale datasets down to significantly smaller sets of (potentially synthetic) training examples, which preserve sufficient information for training a new model from scratch. So far, dataset distillation methods have been developed for image classification. However, with the rise in capabilities of vision-language models (VLMs), and especially given the scale of datasets necessary to train these models, the time is ripe to expand dataset distillation methods beyond image classification. In this work, we take the first steps towards this goal by expanding the idea of trajectory matching to create a distillation method for vision-language datasets. A key challenge is that vision-language datasets do not have a set of discrete classes. To overcome this, our proposed vision-language dataset distillation method jointly distills the image-text pairs in a contrastive formulation. Since there are no existing baselines, we compare our approach to three coreset selection methods (strategic subsampling of the training dataset), which we adapt to the vision-language setting. We demonstrate significant improvements on the challenging Flickr30K and COCO retrieval benchmarks: for example, on Flickr30K, the best coreset selection method selecting 1000 image-text pairs for training achieves only 5.6\% image-to-text retrieval accuracy (i.e., recall@1); in contrast, our dataset distillation approach almost doubles that to 9.9\% with just 100 (an order of magnitude fewer) training pairs.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wu, Xindi and Zhang, Byron and Deng, Zhiwei and Russakovsky, Olga},
	month = oct,
	year = {2023},
	note = {arXiv:2308.07545 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 27 pages, 11 figures},
	file = {Wu et al. - 2023 - Vision-Language Dataset Distillation.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\7NYBHJWA\\Wu et al. - 2023 - Vision-Language Dataset Distillation.pdf:application/pdf},
}

@misc{yang_imagenet-ood_2023,
	title = {{ImageNet}-{OOD}: {Deciphering} {Modern} {Out}-of-{Distribution} {Detection} {Algorithms}},
	shorttitle = {{ImageNet}-{OOD}},
	url = {http://arxiv.org/abs/2310.01755},
	abstract = {The task of out-of-distribution (OOD) detection is notoriously ill-defined. Earlier works focused on new-class detection, aiming to identify label-altering data distribution shifts, also known as "semantic shift." However, recent works argue for a focus on failure detection, expanding the OOD evaluation framework to account for label-preserving data distribution shifts, also known as "covariate shift." Intriguingly, under this new framework, complex OOD detectors that were previously considered state-of-the-art now perform similarly to, or even worse than the simple maximum softmax probability baseline. This raises the question: what are the latest OOD detectors actually detecting? Deciphering the behavior of OOD detection algorithms requires evaluation datasets that decouples semantic shift and covariate shift. To aid our investigations, we present ImageNet-OOD, a clean semantic shift dataset that minimizes the interference of covariate shift. Through comprehensive experiments, we show that OOD detectors are more sensitive to covariate shift than to semantic shift, and the benefits of recent OOD detection algorithms on semantic shift detection is minimal. Our dataset and analyses provide important insights for guiding the design of future OOD detectors.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Yang, William and Zhang, Byron and Russakovsky, Olga},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01755 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 28 pages, 11 figures},
	file = {Yang et al. - 2023 - ImageNet-OOD Deciphering Modern Out-of-Distributi.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\GTD72RHM\\Yang et al. - 2023 - ImageNet-OOD Deciphering Modern Out-of-Distributi.pdf:application/pdf},
}

@misc{zhu_unseen_2023,
	title = {Unseen {Image} {Synthesis} with {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.09213},
	abstract = {While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are distinguishable from the original In-Domain (ID) samples in the intermediate latent spaces, which allows us to sample from them directly. Geometrical domain-specific and model-dependent information of the unseen subspace (e.g., sample-wise distance and angles) is used to further optimize the sampled OOD latent encodings from the estimated Gaussian prior. We conduct extensive analysis and experiments using pre-trained diffusion models (DDPM, iDDPM) on different datasets (AFHQ, CelebA-HQ, LSUN-Church, and LSUN-Bedroom), proving the effectiveness of this novel perspective to explore and re-think the diffusion models' data synthesis generalization ability.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhu, Ye and Wu, Yu and Deng, Zhiwei and Russakovsky, Olga and Yan, Yan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09213 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 28 pages including appendices},
	file = {Zhu et al. - 2023 - Unseen Image Synthesis with Diffusion Models.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\DWFFVMHB\\Zhu et al. - 2023 - Unseen Image Synthesis with Diffusion Models.pdf:application/pdf},
}

@inproceedings{song_objectstitch_2023,
	address = {Vancouver, BC, Canada},
	title = {{ObjectStitch}: {Object} {Compositing} with {Diffusion} {Model}},
	isbn = {9798350301298},
	shorttitle = {{ObjectStitch}},
	url = {https://ieeexplore.ieee.org/document/10203994/},
	doi = {10.1109/CVPR52729.2023.01756},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Song, Yizhi and Zhang, Zhifei and Lin, Zhe and Cohen, Scott and Price, Brian and Zhang, Jianming and Kim, Soo Ye and Aliaga, Daniel},
	month = jun,
	year = {2023},
	pages = {18310--18319},
	file = {Song et al. - 2023 - ObjectStitch Object Compositing with Diffusion Mo.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\VMABMI3P\\Song et al. - 2023 - ObjectStitch Object Compositing with Diffusion Mo.pdf:application/pdf},
}

@article{liu_treepartnet_2021,
	title = {{TreePartNet}: neural decomposition of point clouds for {3D} tree reconstruction},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{TreePartNet}},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480486},
	doi = {10.1145/3478513.3480486},
	abstract = {We present
              TreePartNet
              , a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural
              neural decomposition
              exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.},
	language = {en},
	number = {6},
	urldate = {2023-12-14},
	journal = {ACM Transactions on Graphics},
	author = {Liu, Yanchao and Guo, Jianwei and Benes, Bedrich and Deussen, Oliver and Zhang, Xiaopeng and Huang, Hui},
	month = dec,
	year = {2021},
	pages = {1--16},
	file = {Liu et al. - 2021 - TreePartNet neural decomposition of point clouds .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\9WPB4CVF\\Liu et al. - 2021 - TreePartNet neural decomposition of point clouds .pdf:application/pdf},
}

@misc{wang_score_2022,
	title = {Score {Jacobian} {Chaining}: {Lifting} {Pretrained} {2D} {Diffusion} {Models} for {3D} {Generation}},
	shorttitle = {Score {Jacobian} {Chaining}},
	url = {http://arxiv.org/abs/2212.00774},
	abstract = {A diffusion model learns to predict a vector ﬁeld of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance ﬁeld. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several offthe-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION 5B dataset.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wang, Haochen and Du, Xiaodan and Li, Jiahao and Yeh, Raymond A. and Shakhnarovich, Greg},
	month = dec,
	year = {2022},
	note = {arXiv:2212.00774 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: project page https://pals.ttic.edu/p/score-jacobian-chaining},
	annote = {My questions:


Overall, we assume that 
however in empty loss:
are they compatible?



it seems that the center depth loss can be only applied to one object. How do we generalize to other objects? labeling and cross entropy?


RGB colors are also involved, it seems that we do not supervise that.


},
	file = {Wang et al. - 2022 - Score Jacobian Chaining Lifting Pretrained 2D Dif.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\YMGKWWWM\\Wang et al. - 2022 - Score Jacobian Chaining Lifting Pretrained 2D Dif.pdf:application/pdf},
}

@inproceedings{firoze_tree_2023,
	address = {Vancouver, BC, Canada},
	title = {Tree {Instance} {Segmentation} with {Temporal} {Contour} {Graph}},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10204310/},
	doi = {10.1109/CVPR52729.2023.00218},
	abstract = {We present a novel approach to perform instance segmentation and counting for densely packed self-similar trees using a top-view RGB image sequence. We propose a solution that leverages pixel content, shape, and selfocclusion. First, we perform an initial over-segmentation of the image sequence and aggregate structural characteristics into a contour graph with temporal information incorporated. Second, using a graph convolutional network and its inherent local messaging passing abilities, we merge adjacent tree crown patches into a final set of tree crowns. Per various studies and comparisons, our method is superior to all prior methods and results in high-accuracy instance segmentation and counting despite the trees being tightly packed. Finally, we provide various forest image sequence datasets suitable for subsequent benchmarking and evaluation captured at different altitudes and leaf conditions.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Firoze, Adnan and Wingren, Cameron and Yeh, Raymond A. and Benes, Bedrich and Aliaga, Daniel},
	month = jun,
	year = {2023},
	pages = {2193--2202},
	annote = {My question:
segmented boundaries are not separated from each other? That is not the same in 2D cell membrane segmentation.
How to report the counting number? Do we keep track of them over the overall calculation process or we simply detect?
},
	file = {Firoze et al. - 2023 - Tree Instance Segmentation with Temporal Contour G.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\UZD8NIPY\\Firoze et al. - 2023 - Tree Instance Segmentation with Temporal Contour G.pdf:application/pdf},
}

@misc{lei_efem_2023,
	title = {{EFEM}: {Equivariant} {Neural} {Field} {Expectation} {Maximization} for {3D} {Object} {Segmentation} {Without} {Scene} {Supervision}},
	shorttitle = {{EFEM}},
	url = {http://arxiv.org/abs/2303.15440},
	abstract = {We introduce Equivariant Neural Field Expectation Maximization (EFEM), a simple, effective, and robust geometric algorithm that can segment objects in 3D scenes without annotations or training on scenes. We achieve such unsupervised segmentation by exploiting single object shape priors. We make two novel steps in that direction. First, we introduce equivariant shape representations to this problem to eliminate the complexity induced by the variation in object configuration. Second, we propose a novel EM algorithm that can iteratively refine segmentation masks using the equivariant shape prior. We collect a novel real dataset Chairs and Mugs that contains various object configurations and novel scenes in order to verify the effectiveness and robustness of our method. Experimental results demonstrate that our method achieves consistent and robust performance across different scenes where the (weakly) supervised methods may fail. Code and data available at https://www.cis.upenn.edu/{\textasciitilde}leijh/projects/efem},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Lei, Jiahui and Deng, Congyue and Schmeckpeper, Karl and Guibas, Leonidas and Daniilidis, Kostas},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15440 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR2023, project page https://www.cis.upenn.edu/{\textasciitilde}leijh/projects/efem},
	file = {Lei et al. - 2023 - EFEM Equivariant Neural Field Expectation Maximiz.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\GT7NXVKU\\Lei et al. - 2023 - EFEM Equivariant Neural Field Expectation Maximiz.pdf:application/pdf},
}

@inproceedings{kolotouros_probabilistic_2021,
	address = {Montreal, QC, Canada},
	title = {Probabilistic {Modeling} for {Human} {Mesh} {Recovery}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710873/},
	doi = {10.1109/ICCV48922.2021.01140},
	abstract = {This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing ﬂows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efﬁcient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model ﬁtting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: https://www.seas.upenn.edu/ ˜nkolot/projects/prohmr.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Kolotouros, Nikos and Pavlakos, Georgios and Jayaraman, Dinesh and Daniilidis, Kostas},
	month = oct,
	year = {2021},
	pages = {11585--11594},
	file = {Kolotouros et al. - 2021 - Probabilistic Modeling for Human Mesh Recovery.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\VWU5Z5UP\\Kolotouros et al. - 2021 - Probabilistic Modeling for Human Mesh Recovery.pdf:application/pdf},
}

@misc{gu_control3diff_2023,
	title = {{Control3Diff}: {Learning} {Controllable} {3D} {Diffusion} {Models} from {Single}-view {Images}},
	shorttitle = {{Control3Diff}},
	url = {http://arxiv.org/abs/2304.06700},
	abstract = {Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts. Please see the project website ({\textbackslash}url\{https://jiataogu.me/control3diff\}) for video comparisons.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Gu, Jiatao and Gao, Qingzhe and Zhai, Shuangfei and Chen, Baoquan and Liu, Lingjie and Susskind, Josh},
	month = oct,
	year = {2023},
	note = {arXiv:2304.06700 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by 3DV24},
	file = {Gu et al. - 2023 - Control3Diff Learning Controllable 3D Diffusion M.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\K4R7T7T4\\Gu et al. - 2023 - Control3Diff Learning Controllable 3D Diffusion M.pdf:application/pdf},
}

@misc{zhuang_dreameditor_2023,
	title = {{DreamEditor}: {Text}-{Driven} {3D} {Scene} {Editing} with {Neural} {Fields}},
	shorttitle = {{DreamEditor}},
	url = {http://arxiv.org/abs/2306.13455},
	abstract = {Neural fields have achieved impressive advancements in view synthesis and scene reconstruction. However, editing these neural fields remains challenging due to the implicit encoding of geometry and texture information. In this paper, we propose DreamEditor, a novel framework that enables users to perform controlled editing of neural fields using text prompts. By representing scenes as mesh-based neural fields, DreamEditor allows localized editing within specific regions. DreamEditor utilizes the text encoder of a pretrained text-to-Image diffusion model to automatically identify the regions to be edited based on the semantics of the text prompts. Subsequently, DreamEditor optimizes the editing region and aligns its geometry and texture with the text prompts through score distillation sampling [29]. Extensive experiments have demonstrated that DreamEditor can accurately edit neural fields of real-world scenes according to the given text prompts while ensuring consistency in irrelevant areas. DreamEditor generates highly realistic textures and geometry, significantly surpassing previous works in both quantitative and qualitative evaluations.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhuang, Jingyu and Wang, Chen and Liu, Lingjie and Lin, Liang and Li, Guanbin},
	month = sep,
	year = {2023},
	note = {arXiv:2306.13455 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by SIGGRAPH Asia 2023},
	file = {Zhuang et al. - 2023 - DreamEditor Text-Driven 3D Scene Editing with Neu.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\A82EXJDV\\Zhuang et al. - 2023 - DreamEditor Text-Driven 3D Scene Editing with Neu.pdf:application/pdf},
}

@misc{po_state_2023,
	title = {State of the {Art} on {Diffusion} {Models} for {Visual} {Computing}},
	url = {http://arxiv.org/abs/2310.07204},
	abstract = {The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Po, Ryan and Yifan, Wang and Golyanik, Vladislav and Aberman, Kfir and Barron, Jonathan T. and Bermano, Amit H. and Chan, Eric Ryan and Dekel, Tali and Holynski, Aleksander and Kanazawa, Angjoo and Liu, C. Karen and Liu, Lingjie and Mildenhall, Ben and Nießner, Matthias and Ommer, Björn and Theobalt, Christian and Wonka, Peter and Wetzstein, Gordon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07204 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {Po et al. - 2023 - State of the Art on Diffusion Models for Visual Co.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\XDNZB5VW\\Po et al. - 2023 - State of the Art on Diffusion Models for Visual Co.pdf:application/pdf},
}

@misc{long_wonder3d_2023,
	title = {{Wonder3D}: {Single} {Image} to {3D} using {Cross}-{Domain} {Diffusion}},
	shorttitle = {{Wonder3D}},
	url = {http://arxiv.org/abs/2310.15008},
	abstract = {In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Long, Xiaoxiao and Guo, Yuan-Chen and Lin, Cheng and Liu, Yuan and Dou, Zhiyang and Liu, Lingjie and Ma, Yuexin and Zhang, Song-Hai and Habermann, Marc and Theobalt, Christian and Wang, Wenping},
	month = nov,
	year = {2023},
	note = {arXiv:2310.15008 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project page: https://www.xxlong.site/Wonder3D/},
	file = {Long et al. - 2023 - Wonder3D Single Image to 3D using Cross-Domain Di.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\XM76YLXI\\Long et al. - 2023 - Wonder3D Single Image to 3D using Cross-Domain Di.pdf:application/pdf},
}

@incollection{whitton_modeling_2023,
	address = {New York, NY, USA},
	edition = {1},
	title = {Modeling and {Rendering} {Architecture} from {Photographs}: {A} hybrid geometry- and image-based approach},
	isbn = {9798400708978},
	shorttitle = {Modeling and {Rendering} {Architecture} from {Photographs}},
	url = {https://dl.acm.org/doi/10.1145/3596711.3596761},
	abstract = {We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The ﬁrst component is a photogrammetric modeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach’s ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Seminal {Graphics} {Papers}: {Pushing} the {Boundaries}, {Volume} 2},
	publisher = {ACM},
	author = {Debevec, Paul E. and Taylor, Camillo J. and Malik, Jitendra},
	editor = {Whitton, Mary C.},
	month = aug,
	year = {2023},
	doi = {10.1145/3596711.3596761},
	pages = {465--474},
	file = {Debevec et al. - 2023 - Modeling and Rendering Architecture from Photograp.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\HJU7PRNK\\Debevec et al. - 2023 - Modeling and Rendering Architecture from Photograp.pdf:application/pdf},
}

@incollection{michalowski_hierarchical_2022,
	address = {Cham},
	title = {Hierarchical {Deep} {Multi}-task {Learning} for {Classification} of {Patient} {Diagnoses}},
	volume = {13263},
	isbn = {978-3-031-09341-8 978-3-031-09342-5},
	url = {https://link.springer.com/10.1007/978-3-031-09342-5_12},
	abstract = {Recent years have witnessed an increased interest in the biomedical research community in developing machine learning models and methods that can automatically assign diagnostic codes (ICD) to patient stays based on the information in their Electronic Health Records (EHR). However, despite the recent advances, accurate automatic classiﬁcation of diagnostic codes continues to face challenges, especially for low-prior diagnostic codes. To alleviate the problem, we propose to leverage information in the diagnostic hierarchy and better utilize the dependencies among diseases in this hierarchy. We develop a new hierarchical deep multi-task learning method that learns classiﬁcation models for multiple diagnostic codes at the diﬀerent levels of abstraction in the disease hierarchy while allowing the transfer of information from highlevel nodes, more general diagnoses codes to the low-level ones, more speciﬁc diagnostic codes. After that, we reﬁne the initial hierarchical model by utilizing the relations and information that can discriminate better between competing diseases. Our empirical results show that our new method and its reﬁnement outperform baseline machine learning architectures that do not leverage the hierarchical structure of target diagnoses tasks or disease-disease relationships.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Malakouti, Salim and Hauskrecht, Milos},
	editor = {Michalowski, Martin and Abidi, Syed Sibte Raza and Abidi, Samina},
	year = {2022},
	doi = {10.1007/978-3-031-09342-5_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {122--132},
	file = {Malakouti and Hauskrecht - 2022 - Hierarchical Deep Multi-task Learning for Classifi.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\ZZZMIYQ9\\Malakouti and Hauskrecht - 2022 - Hierarchical Deep Multi-task Learning for Classifi.pdf:application/pdf},
}

@incollection{michalowski_learning_2022,
	address = {Cham},
	title = {Learning to {Adapt} {Dynamic} {Clinical} {Event} {Sequences} with {Residual} {Mixture} of {Experts}},
	volume = {13263},
	isbn = {978-3-031-09341-8 978-3-031-09342-5},
	url = {https://link.springer.com/10.1007/978-3-031-09342-5_15},
	abstract = {Clinical event sequences in Electronic Health Records (EHRs) record detailed information about the patient condition and patient care as they occur in time. Recent years have witnessed increased interest of machine learning community in developing machine learning models solving diﬀerent types of problems deﬁned upon information in EHRs. More recently, neural sequential models, such as RNN and LSTM, became popular and widely applied models for representing patient sequence data and for predicting future events or outcomes based on such data. However, a single neural sequential model may not properly represent complex dynamics of all patients and the diﬀerences in their behaviors. In this work, we aim to alleviate this limitation by reﬁning a one-ﬁts-all model using a Mixture-of-Experts (MoE) architecture. The architecture consists of multiple (expert) RNN models covering patient sub-populations and reﬁning the predictions of the base model. That is, instead of training expert RNN models from scratch we deﬁne them on the residual signal that attempts to model the diﬀerences from the population-wide model. The heterogeneity of various patient sequences is modeled through multiple experts that consist of RNN. Particularly, instead of directly training MoE from scratch, we augment MoE based on the prediction signal from pretrained base GRU model. With this way, the mixture of experts can provide ﬂexible adaptation to the (limited) predictive power of the single base RNN model. We experiment with the newly proposed model on realworld EHRs data and the multivariate clinical event prediction task. We implement RNN using Gated Recurrent Units (GRU). We show 4.1\% gain on AUPRC statistics compared to a single RNN prediction.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Lee, Jeong Min and Hauskrecht, Milos},
	editor = {Michalowski, Martin and Abidi, Syed Sibte Raza and Abidi, Samina},
	year = {2022},
	doi = {10.1007/978-3-031-09342-5_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {155--166},
	file = {Lee and Hauskrecht - 2022 - Learning to Adapt Dynamic Clinical Event Sequences.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\MQ3YRGAW\\Lee and Hauskrecht - 2022 - Learning to Adapt Dynamic Clinical Event Sequences.pdf:application/pdf},
}

@article{aboutalib_deep_2018,
	title = {Deep {Learning} to {Distinguish} {Recalled} but {Benign} {Mammography} {Images} in {Breast} {Cancer} {Screening}},
	volume = {24},
	issn = {1078-0432, 1557-3265},
	url = {https://aacrjournals.org/clincancerres/article/24/23/5902/81079/Deep-Learning-to-Distinguish-Recalled-but-Benign},
	doi = {10.1158/1078-0432.CCR-18-1115},
	abstract = {Purpose: False positives in digital mammography screening lead to high recall rates, resulting in unnecessary medical procedures to patients and health care costs. This study aimed to investigate the revolutionary deep learning methods to distinguish recalled but benign mammography images from negative exams and those with malignancy.},
	language = {en},
	number = {23},
	urldate = {2023-12-14},
	journal = {Clinical Cancer Research},
	author = {Aboutalib, Sarah S. and Mohamed, Aly A. and Berg, Wendie A. and Zuley, Margarita L. and Sumkin, Jules H. and Wu, Shandong},
	month = dec,
	year = {2018},
	pages = {5902--5909},
	file = {Aboutalib et al. - 2018 - Deep Learning to Distinguish Recalled but Benign M.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\L5W8PKV2\\Aboutalib et al. - 2018 - Deep Learning to Distinguish Recalled but Benign M.pdf:application/pdf},
}

@misc{gungor_boosting_2023,
	title = {Boosting {Weakly} {Supervised} {Object} {Detection} using {Fusion} and {Priors} from {Hallucinated} {Depth}},
	url = {http://arxiv.org/abs/2303.10937},
	abstract = {Despite recent attention to depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to different WSOD methods based on multiple-instance learning, without necessitating additional annotations or inducing large computational cost. Our proposed method employs monocular depth estimation to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of per-box predictions. We evaluate our proposed method on three datasets (COCO, PASCAL VOC, and Conceptual Captions) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Gungor, Cagri and Kovashka, Adriana},
	month = nov,
	year = {2023},
	note = {arXiv:2303.10937 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gungor and Kovashka - 2023 - Boosting Weakly Supervised Object Detection using .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\98BY8DPM\\Gungor and Kovashka - 2023 - Boosting Weakly Supervised Object Detection using .pdf:application/pdf},
}

@misc{malakouti_semi-supervised_2023,
	title = {Semi-{Supervised} {Domain} {Generalization} for {Object} {Detection} via {Language}-{Guided} {Feature} {Alignment}},
	url = {http://arxiv.org/abs/2309.13525},
	abstract = {Existing domain adaptation (DA) and generalization (DG) methods in object detection enforce feature alignment in the visual space but face challenges like object appearance variability and scene complexity, which make it difficult to distinguish between objects and achieve accurate detection. In this paper, we are the first to address the problem of semi-supervised domain generalization by exploring vision-language pre-training and enforcing feature alignment through the language space. We employ a novel CrossDomain Descriptive Multi-Scale Learning (CDDMSL) aiming to maximize the agreement between descriptions of an image presented with different domain-specific characteristics in the embedding space. CDDMSL significantly outperforms existing methods, achieving 11.7\% and 7.5\% improvement in DG and DA settings, respectively. Comprehensive analysis and ablation studies confirm the effectiveness of our method, positioning CDDMSL as a promising approach for domain generalization in object detection tasks. Our code is available at https://github.com/sinamalakouti/CDDMSL.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Malakouti, Sina and Kovashka, Adriana},
	month = sep,
	year = {2023},
	note = {arXiv:2309.13525 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at BMVC 2023},
	file = {Malakouti and Kovashka - 2023 - Semi-Supervised Domain Generalization for Object D.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\I8NDGQYZ\\Malakouti and Kovashka - 2023 - Semi-Supervised Domain Generalization for Object D.pdf:application/pdf},
}

@inproceedings{nebbia_hypernymization_2023,
	address = {Thessaloniki Greece},
	title = {Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining},
	isbn = {9798400701788},
	url = {https://dl.acm.org/doi/10.1145/3591106.3592223},
	doi = {10.1145/3591106.3592223},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 2023 {ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Nebbia, Giacomo and Kovashka, Adriana},
	month = jun,
	year = {2023},
	pages = {67--75},
	file = {Nebbia and Kovashka - 2023 - Hypernymization of named entity-rich captions for .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NBQQDGM8\\Nebbia and Kovashka - 2023 - Hypernymization of named entity-rich captions for .pdf:application/pdf},
}

@inproceedings{rai_improving_2023,
	address = {Vancouver, BC, Canada},
	title = {Improving language-supervised object detection with linguistic structure analysis},
	isbn = {9798350302493},
	url = {https://ieeexplore.ieee.org/document/10208402/},
	doi = {10.1109/CVPRW59228.2023.00588},
	abstract = {Language-supervised object detection typically uses descriptive captions from human-annotated datasets. However, in-the-wild captions take on wider styles of language. We analyze one particular ubiquitous form of language: narrative. We study the differences in linguistic structure and visual-text alignment in narrative and descriptive captions and find we can classify descriptive and narrative style captions using linguistic features such as part of speech, rhetoric structure theory, and multimodal discourse. Then, we use this to select captions from which to extract imagelevel labels as supervision for weakly supervised object detection. We also improve the quality of extracted labels by filtering based on proximity to verb types for both descriptive and narrative captions.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Rai, Arushi and Kovashka, Adriana},
	month = jun,
	year = {2023},
	pages = {5560--5570},
	file = {Rai and Kovashka - 2023 - Improving language-supervised object detection wit.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\HUFEWCVT\\Rai and Kovashka - 2023 - Improving language-supervised object detection wit.pdf:application/pdf},
}

@incollection{greenspan_colossal_2023,
	address = {Cham},
	title = {{COLosSAL}: {A} {Benchmark} for {Cold}-{Start} {Active} {Learning} for {3D} {Medical} {Image} {Segmentation}},
	volume = {14221},
	isbn = {978-3-031-43894-3 978-3-031-43895-0},
	shorttitle = {{COLosSAL}},
	url = {https://link.springer.com/10.1007/978-3-031-43895-0_3},
	abstract = {Medical image segmentation is a critical task in medical image analysis. In recent years, deep learning based approaches have shown exceptional performance when trained on a fully-annotated dataset. However, data annotation is often a signiﬁcant bottleneck, especially for 3D medical images. Active learning (AL) is a promising solution for eﬃcient annotation but requires an initial set of labeled samples to start active selection. When the entire data pool is unlabeled, how do we select the samples to annotate as our initial set? This is also known as the cold-start AL, which permits only one chance to request annotations from experts without access to previously annotated data. Cold-start AL is highly relevant in many practical scenarios but has been under-explored, especially for 3D medical segmentation tasks requiring substantial annotation eﬀort. In this paper, we present a benchmark named COLosSAL by evaluating six cold-start AL strategies on ﬁve 3D medical image segmentation tasks from the public Medical Segmentation Decathlon collection. We perform a thorough performance analysis and explore important open questions for cold-start AL, such as the impact of budget on diﬀerent strategies. Our results show that cold-start AL is still an unsolved problem for 3D segmentation tasks but some important trends have been observed. The code repository, data partitions, and baseline results for the complete benchmark are publicly available at https://github.com/MedICL-VU/COLosSAL.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Liu, Han and Li, Hao and Yao, Xing and Fan, Yubo and Hu, Dewei and Dawant, Benoit M. and Nath, Vishwesh and Xu, Zhoubing and Oguz, Ipek},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	doi = {10.1007/978-3-031-43895-0_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {25--34},
	file = {Liu et al. - 2023 - COLosSAL A Benchmark for Cold-Start Active Learni.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\XHNM68GH\\Liu et al. - 2023 - COLosSAL A Benchmark for Cold-Start Active Learni.pdf:application/pdf},
}

@misc{hu_map_2023,
	title = {{MAP}: {Domain} {Generalization} via {Meta}-{Learning} on {Anatomy}-{Consistent} {Pseudo}-{Modalities}},
	shorttitle = {{MAP}},
	url = {http://arxiv.org/abs/2309.01286},
	abstract = {Deep models suffer from limited generalization capability to unseen domains, which has severely hindered their clinical applicability. Specifically for the retinal vessel segmentation task, although the model is supposed to learn the anatomy of the target, it can be distracted by confounding factors like intensity and contrast. We propose Meta learning on Anatomy-consistent Pseudo-modalities (MAP), a method that improves model generalizability by learning structural features. We first leverage a feature extraction network to generate three distinct pseudo-modalities that share the vessel structure of the original image. Next, we use the episodic learning paradigm by selecting one of the pseudo-modalities as the meta-train dataset, and perform meta-testing on a continuous augmented image space generated through Dirichlet mixup of the remaining pseudo-modalities. Further, we introduce two loss functions that facilitate the model’s focus on shape information by clustering the latent vectors obtained from images featuring identical vasculature. We evaluate our model on seven public datasets of various retinal imaging modalities and we conclude that MAP has substantially better generalizability. Our code is publically available at https://github.com/DeweiHu/MAP.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Hu, Dewei and Li, Hao and Liu, Han and Yao, Xing and Wang, Jiacheng and Oguz, Ipek},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01286 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hu et al. - 2023 - MAP Domain Generalization via Meta-Learning on An.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\VZQDNM4U\\Hu et al. - 2023 - MAP Domain Generalization via Meta-Learning on An.pdf:application/pdf},
}

@article{doss_508_2023,
	title = {508 {Deep} {Learning} {Segmentation} of the {Nucleus} {Basalis} of {Meynert} for {Deep} {Brain} {Stimulation} {Surgical} {Planning}},
	volume = {69},
	issn = {0148-396X, 1524-4040},
	url = {https://journals.lww.com/10.1227/neu.0000000000002375_508},
	doi = {10.1227/neu.0000000000002375_508},
	abstract = {INTRODUCTION: The nucleus basalis of Meynert (NBM) is a subcortical structure involved in arousal and cognition being explored as a deep brain stimulation (DBS) target for Alzheimer’s disease and other disorders. Given the small size of the NBM and variability between patients, accurate patient-speciﬁc targeting for DBS is needed. Accurate patient-speciﬁc manual segmentation is possible but requires high resolution 7T MRI. Therefore, the current standard is a non-patient-speciﬁc probabilistic atlas (Zaborszky 2008).
METHODS: Paired 3T and 7T MRI datasets of 21 healthy subjects were obtained and the NBM was expertly segmented on 7T MRI. The 7T NBM segmentation was then used on the 3T MRI. To increase generalizability, we augmented the dataset to a total of 210 images. An external dataset of 14 patients with temporal lobe epilepsy (TLE) was used to validate the network’s performance on brains with pathological changes (Alvim 2016). A 3D-Unet convolutional neural network was constructed, and a 5-fold cross-validation was performed for model selection. The model was evaluated on healthy subjects using the held-out test dataset and on the external dataset of 14 TLE patients.
RESULTS: When tested on the held-out dataset, the deep learning network demonstrated signiﬁcantly improved dice coefﬁcient compared against the probabilistic atlas for both healthy subjects (0.68 ± 0.08, 0.47 ± 0.06) and patients with TLE (0.63 ± 0.08, 0.38 ± 0.19). Additionally, the centroid distance was signiﬁcantly decreased when using the network in patients with TLE (1.22 ± 0.33 mm, 3.25 ± 2.57 mm).
CONCLUSIONS: We developed the ﬁrst network, to our knowledge, for automatic and accurate patient-speciﬁc segmentation of the NBM using deep learning. The proposed network has less error than accepted targeting margins. This segmentation strategy allows accurate patient-speciﬁc targeting of the NBM for DBS.},
	language = {en},
	number = {Supplement\_1},
	urldate = {2023-12-14},
	journal = {Neurosurgery},
	author = {Doss, Derek and Johnson, Graham Walter and Narasimhan, Saramati and Jiang, Jasmine and Gonzalez, Hernan F.J. and Paulo, Danika Lea and Chang, Catie and Morgan, Victoria and Constantinidis, Christos and Dawant, Benoit M. and Englot, Dario J.},
	month = apr,
	year = {2023},
	pages = {112--113},
	file = {Doss et al. - 2023 - 508 Deep Learning Segmentation of the Nucleus Basa.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\7ZKHRRCL\\Doss et al. - 2023 - 508 Deep Learning Segmentation of the Nucleus Basa.pdf:application/pdf},
}

@misc{said_neurograph_2023,
	title = {{NeuroGraph}: {Benchmarks} for {Graph} {Machine} {Learning} in {Brain} {Connectomics}},
	shorttitle = {{NeuroGraph}},
	url = {http://arxiv.org/abs/2306.06202},
	abstract = {Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by crafting 35 datasets within both static and dynamic contexts, running in excess of 15 baseline methods for benchmarking. Additionally, we provide generic frameworks for learning on dynamic as well as static graphs. Our extensive experiments lead to several key observations. Notably, using correlation vectors as node features, incorporating larger number of regions of interest, and employing sparser graphs lead to improved performance. To foster further advancements in graph-based data driven Neuroimaging, we offer a comprehensive open source Python package that includes the datasets, baseline implementations, model training, and standard evaluation. The package is publicly accessible at https://anwar-said.github.io/anwarsaid/neurograph.html.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Said, Anwar and Bayrak, Roza G. and Derr, Tyler and Shabbir, Mudassir and Moyer, Daniel and Chang, Catie and Koutsoukos, Xenofon},
	month = nov,
	year = {2023},
	note = {arXiv:2306.06202 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: NeurIPS23},
	file = {Said et al. - 2023 - NeuroGraph Benchmarks for Graph Machine Learning .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\7DIISJL3\\Said et al. - 2023 - NeuroGraph Benchmarks for Graph Machine Learning .pdf:application/pdf},
}

@article{nanda_time-resolved_2023,
	title = {Time-resolved correlation of distributed brain activity tracks {E}-{I} balance and accounts for diverse scale-free phenomena},
	volume = {42},
	issn = {22111247},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2211124723002656},
	doi = {10.1016/j.celrep.2023.112254},
	abstract = {Much of systems neuroscience posits the functional importance of brain activity patterns that lack natural scales of sizes, durations, or frequencies. The ﬁeld has developed prominent, and sometimes competing, explanations for the nature of this scale-free activity. Here, we reconcile these explanations across species and modalities. First, we link estimates of excitation-inhibition (E-I) balance with time-resolved correlation of distributed brain activity. Second, we develop an unbiased method for sampling time series constrained by this time-resolved correlation. Third, we use this method to show that estimates of E-I balance account for diverse scale-free phenomena without need to attribute additional function or importance to these phenomena. Collectively, our results simplify existing explanations of scale-free brain activity and provide stringent tests on future theories that seek to transcend these explanations.},
	language = {en},
	number = {4},
	urldate = {2023-12-14},
	journal = {Cell Reports},
	author = {Nanda, Aditya and Johnson, Graham W. and Mu, Yu and Ahrens, Misha B. and Chang, Catie and Englot, Dario J. and Breakspear, Michael and Rubinov, Mikail},
	month = apr,
	year = {2023},
	pages = {112254},
	file = {Nanda et al. - 2023 - Time-resolved correlation of distributed brain act.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\LA49RYRD\\Nanda et al. - 2023 - Time-resolved correlation of distributed brain act.pdf:application/pdf},
}

@article{yoo_multimodal_2023,
	title = {Multimodal neuroimaging data from a 5-week heart rate variability biofeedback randomized clinical trial},
	volume = {10},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-02396-5},
	doi = {10.1038/s41597-023-02396-5},
	abstract = {Abstract
            
              We present data from the Heart Rate Variability and Emotion Regulation (HRV-ER) randomized clinical trial testing effects of HRV biofeedback. Younger (N = 121) and older (N = 72) participants completed baseline magnetic resonance imaging (MRI) including T
              1
              -weighted, resting and emotion regulation task functional MRI (fMRI), pulsed continuous arterial spin labeling (PCASL), and proton magnetic resonance spectroscopy (
              1
              H MRS). During fMRI scans, physiological measures (blood pressure, pulse, respiration, and end-tidal CO
              2
              ) were continuously acquired. Participants were randomized to either increase heart rate oscillations or decrease heart rate oscillations during daily sessions. After 5 weeks of HRV biofeedback, they repeated the baseline measurements in addition to new measures (ultimatum game fMRI, training mimicking during blood oxygen level dependent (BOLD) and PCASL fMRI). Participants also wore a wristband sensor to estimate sleep time. Psychological assessment comprised three cognitive tests and ten questionnaires related to emotional well-being. A subset (N = 104) provided plasma samples pre- and post-intervention that were assayed for amyloid and tau. Data is publicly available via the OpenNeuro data sharing platform.},
	language = {en},
	number = {1},
	urldate = {2023-12-14},
	journal = {Scientific Data},
	author = {Yoo, Hyun Joo and Nashiro, Kaoru and Min, Jungwon and Cho, Christine and Mercer, Noah and Bachman, Shelby L. and Nasseri, Padideh and Dutt, Shubir and Porat, Shai and Choi, Paul and Zhang, Yong and Grigoryan, Vardui and Feng, Tiantian and Thayer, Julian F. and Lehrer, Paul and Chang, Catie and Stanley, Jeffrey A. and Head, Elizabeth and Rouanet, Jeremy and Marmarelis, Vasilis Z. and Narayanan, Shrikanth and Wisnowski, Jessica and Nation, Daniel A. and Mather, Mara},
	month = jul,
	year = {2023},
	pages = {503},
	file = {Yoo et al. - 2023 - Multimodal neuroimaging data from a 5-week heart r.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\6JM62HA2\\Yoo et al. - 2023 - Multimodal neuroimaging data from a 5-week heart r.pdf:application/pdf},
}

@misc{zia_surgical_2023,
	title = {Surgical tool classification and localization: results and methods from the {MICCAI} 2022 {SurgToolLoc} challenge},
	shorttitle = {Surgical tool classification and localization},
	url = {http://arxiv.org/abs/2305.07152},
	abstract = {The ability to automatically detect and track surgical instruments in endoscopic videos can enable transformational interventions. Assessing surgical performance and efficiency, identifying skilled tool use and choreography, and planning operational and logistical aspects of OR resources are just a few of the applications that could benefit. Unfortunately, obtaining the annotations needed to train machine learning models to identify and localize surgical tools is a difficult task. Annotating bounding boxes frame-by-frame is tedious and time-consuming, yet large amounts of data with a wide variety of surgical tools and surgeries must be captured for robust training. Moreover, ongoing annotator training is needed to stay up to date with surgical instrument innovation. In robotic-assisted surgery, however, potentially informative data like timestamps of instrument installation and removal can be programmatically harvested. The ability to rely on tool installation data alone would significantly reduce the workload to train robust tool-tracking models. With this motivation in mind we invited the surgical data science community to participate in the challenge, SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels for machine learning models trained to detect tools and localize them in video frames with bounding boxes. We present the results of this challenge along with many of the team's efforts. We conclude by discussing these results in the broader context of machine learning and surgical data science. The training data used for this challenge consisting of 24,695 video clips with tool presence labels is also being released publicly and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zia, Aneeq and Bhattacharyya, Kiran and Liu, Xi and Berniker, Max and Wang, Ziheng and Nespolo, Rogerio and Kondo, Satoshi and Kasai, Satoshi and Hirasawa, Kousuke and Liu, Bo and Austin, David and Wang, Yiheng and Futrega, Michal and Puget, Jean-Francois and Li, Zhenqiang and Sato, Yoichi and Fujii, Ryo and Hachiuma, Ryo and Masuda, Mana and Saito, Hideo and Wang, An and Xu, Mengya and Islam, Mobarakol and Bai, Long and Pang, Winnie and Ren, Hongliang and Nwoye, Chinedu and Sestini, Luca and Padoy, Nicolas and Nielsen, Maximilian and Schüttler, Samuel and Sentker, Thilo and Husseini, Hümeyra and Baltruschat, Ivo and Schmitz, Rüdiger and Werner, René and Matsun, Aleksandr and Farooq, Mugariya and Saaed, Numan and Viera, Jose Renato Restom and Yaqub, Mohammad and Getty, Neil and Xia, Fangfang and Zhao, Zixuan and Duan, Xiaotian and Yao, Xing and Lou, Ange and Yang, Hao and Han, Jintong and Noble, Jack and Wu, Jie Ying and Alshirbaji, Tamer Abdulbaki and Jalal, Nour Aldeen and Arabian, Herag and Ding, Ning and Moeller, Knut and Chen, Weiliang and He, Quan and Bilal, Muhammad and Akinosho, Taofeek and Qayyum, Adnan and Caputo, Massimo and Vohra, Hunaid and Loizou, Michael and Ajayi, Anuoluwapo and Berrou, Ilhem and Niyi-Odumosu, Faatihah and Maier-Hein, Lena and Stoyanov, Danail and Speidel, Stefanie and Jarc, Anthony},
	month = may,
	year = {2023},
	note = {arXiv:2305.07152 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zia et al. - 2023 - Surgical tool classification and localization res.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\QUQ92QHV\\Zia et al. - 2023 - Surgical tool classification and localization res.pdf:application/pdf},
}

@techreport{lu_assist-u_2023,
	type = {preprint},
	title = {{ASSIST}-{U}: {A} {System} for {Segmentation} and {Image} {Style} {Transfer} for {Ureteroscopy}},
	shorttitle = {{ASSIST}-{U}},
	url = {https://www.authorea.com/users/698001/articles/686034-assist-u-a-system-for-segmentation-and-image-style-transfer-for-ureteroscopy?commit=7c9771cf15bde0b4ce04f62604b97a6feb07b0e2},
	abstract = {Kidney stones require surgical removal when they grow too large to be broken up externally or to pass on their own. Upper tract urothelial carcinoma are also sometimes treated endoscopically in a similar procedure. These surgeries are diﬃcult, particularly for trainees who often miss tumors, stones or stone fragments, requiring re-operation. One cause of diﬃculty is the high cognitive strain surgeons experience in creating accurate mental models during the endoscopic operation. Furthermore, there are no patient-speciﬁc simulators to facilitate training or standardized visualization tools for ureteroscopy despite its high prevalence. We propose ASSIST-U, a system to automatically create realistic ureteroscopy images and videos solely using preoperative CT images to address these unmet needs. We train a 3D UNet model to automatically segment CT images and construct 3D surfaces. These surfaces are then skeletonized for rendering and camera position tracking. Finally, we train a style transfer model using Contrastive Unpaired Translation (CUT) to synthesize realistic ureteroscopy images. Cross validation on the UNet model achieved a Dice score of 0.853 \${\textbackslash}pm\$ 0.084 for the CT segmentation step. CUT style transfer produced visually plausible images; the Kernel Inception Distance to real ureteroscopy images was reduced from 0.198 (rendered) to 0.089 (synthesized). We also qualitatively demonstrate the entire pipeline from CT to synthesized ureteroscopy. The proposed ASSIST-U system shows promise for aiding surgeons in visualization of kidney ureteroscopy.},
	language = {en},
	urldate = {2023-12-14},
	institution = {Preprints},
	author = {Lu, Daiwei and Wu, Yifan and Acar, Ayberk and Yao, Xing and Wu, Jie Ying and Kavoussi, Nicholas and Oguz, Ipek},
	month = nov,
	year = {2023},
	doi = {10.22541/au.169960983.39481168/v1},
	file = {Lu et al. - 2023 - ASSIST-U A System for Segmentation and Image Styl.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\TLVHI7YV\\Lu et al. - 2023 - ASSIST-U A System for Segmentation and Image Styl.pdf:application/pdf},
}

@article{wu_cross-modal_2021,
	title = {Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery},
	volume = {16},
	issn = {1861-6410, 1861-6429},
	url = {https://link.springer.com/10.1007/s11548-021-02343-y},
	doi = {10.1007/s11548-021-02343-y},
	abstract = {Purpose Multi- and cross-modal learning consolidates information from multiple data sources which may offer a holistic representation of complex scenarios. Cross-modal learning is particularly interesting, because synchronized data streams are immediately useful as self-supervisory signals. The prospect of achieving self-supervised continual learning in surgical robotics is exciting as it may enable lifelong learning that adapts to different surgeons and cases, ultimately leading to a more general machine understanding of surgical processes.
Methods We present a learning paradigm using synchronous video and kinematics from robot-mediated surgery. Our approach relies on an encoder–decoder network that maps optical ﬂow to the corresponding kinematics sequence. Clustering on the latent representations reveals meaningful groupings for surgeon gesture and skill level. We demonstrate the generalizability of the representations on the JIGSAWS dataset by classifying skill and gestures on tasks not used for training.
Results For tasks seen in training, we report a 59 to 70\% accuracy in surgical gestures classiﬁcation. On tasks beyond the training setup, we note a 45 to 65\% accuracy. Qualitatively, we ﬁnd that unseen gestures form clusters in the latent space of novice actions, which may enable the automatic identiﬁcation of novel interactions in a lifelong learning scenario.
Conclusion From predicting the synchronous kinematics sequence, optical ﬂow representations of surgical scenes emerge that separate well even for new tasks that the model had not seen before. While the representations are useful immediately for a variety of tasks, the self-supervised learning paradigm may enable research in lifelong and user-speciﬁc learning.},
	language = {en},
	number = {5},
	urldate = {2023-12-14},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Wu, Jie Ying and Tamhane, Aniruddha and Kazanzides, Peter and Unberath, Mathias},
	month = may,
	year = {2021},
	pages = {779--787},
	file = {Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\XFBNMNKX\\Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:application/pdf},
}

@misc{zia_surgical_2023-1,
	title = {Surgical tool classification and localization: results and methods from the {MICCAI} 2022 {SurgToolLoc} challenge},
	shorttitle = {Surgical tool classification and localization},
	url = {http://arxiv.org/abs/2305.07152},
	abstract = {The ability to automatically detect and track surgical instruments in endoscopic videos can enable transformational interventions. Assessing surgical performance and efficiency, identifying skilled tool use and choreography, and planning operational and logistical aspects of OR resources are just a few of the applications that could benefit. Unfortunately, obtaining the annotations needed to train machine learning models to identify and localize surgical tools is a difficult task. Annotating bounding boxes frame-by-frame is tedious and time-consuming, yet large amounts of data with a wide variety of surgical tools and surgeries must be captured for robust training. Moreover, ongoing annotator training is needed to stay up to date with surgical instrument innovation. In robotic-assisted surgery, however, potentially informative data like timestamps of instrument installation and removal can be programmatically harvested. The ability to rely on tool installation data alone would significantly reduce the workload to train robust tool-tracking models. With this motivation in mind we invited the surgical data science community to participate in the challenge, SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels for machine learning models trained to detect tools and localize them in video frames with bounding boxes. We present the results of this challenge along with many of the team's efforts. We conclude by discussing these results in the broader context of machine learning and surgical data science. The training data used for this challenge consisting of 24,695 video clips with tool presence labels is also being released publicly and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zia, Aneeq and Bhattacharyya, Kiran and Liu, Xi and Berniker, Max and Wang, Ziheng and Nespolo, Rogerio and Kondo, Satoshi and Kasai, Satoshi and Hirasawa, Kousuke and Liu, Bo and Austin, David and Wang, Yiheng and Futrega, Michal and Puget, Jean-Francois and Li, Zhenqiang and Sato, Yoichi and Fujii, Ryo and Hachiuma, Ryo and Masuda, Mana and Saito, Hideo and Wang, An and Xu, Mengya and Islam, Mobarakol and Bai, Long and Pang, Winnie and Ren, Hongliang and Nwoye, Chinedu and Sestini, Luca and Padoy, Nicolas and Nielsen, Maximilian and Schüttler, Samuel and Sentker, Thilo and Husseini, Hümeyra and Baltruschat, Ivo and Schmitz, Rüdiger and Werner, René and Matsun, Aleksandr and Farooq, Mugariya and Saaed, Numan and Viera, Jose Renato Restom and Yaqub, Mohammad and Getty, Neil and Xia, Fangfang and Zhao, Zixuan and Duan, Xiaotian and Yao, Xing and Lou, Ange and Yang, Hao and Han, Jintong and Noble, Jack and Wu, Jie Ying and Alshirbaji, Tamer Abdulbaki and Jalal, Nour Aldeen and Arabian, Herag and Ding, Ning and Moeller, Knut and Chen, Weiliang and He, Quan and Bilal, Muhammad and Akinosho, Taofeek and Qayyum, Adnan and Caputo, Massimo and Vohra, Hunaid and Loizou, Michael and Ajayi, Anuoluwapo and Berrou, Ilhem and Niyi-Odumosu, Faatihah and Maier-Hein, Lena and Stoyanov, Danail and Speidel, Stefanie and Jarc, Anthony},
	month = may,
	year = {2023},
	note = {arXiv:2305.07152 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zia et al. - 2023 - Surgical tool classification and localization res.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\6H82TGIY\\Zia et al. - 2023 - Surgical tool classification and localization res.pdf:application/pdf},
}

@techreport{lu_assist-u_2023-1,
	type = {preprint},
	title = {{ASSIST}-{U}: {A} {System} for {Segmentation} and {Image} {Style} {Transfer} for {Ureteroscopy}},
	shorttitle = {{ASSIST}-{U}},
	url = {https://www.authorea.com/users/698001/articles/686034-assist-u-a-system-for-segmentation-and-image-style-transfer-for-ureteroscopy?commit=7c9771cf15bde0b4ce04f62604b97a6feb07b0e2},
	abstract = {Kidney stones require surgical removal when they grow too large to be broken up externally or to pass on their own. Upper tract urothelial carcinoma are also sometimes treated endoscopically in a similar procedure. These surgeries are diﬃcult, particularly for trainees who often miss tumors, stones or stone fragments, requiring re-operation. One cause of diﬃculty is the high cognitive strain surgeons experience in creating accurate mental models during the endoscopic operation. Furthermore, there are no patient-speciﬁc simulators to facilitate training or standardized visualization tools for ureteroscopy despite its high prevalence. We propose ASSIST-U, a system to automatically create realistic ureteroscopy images and videos solely using preoperative CT images to address these unmet needs. We train a 3D UNet model to automatically segment CT images and construct 3D surfaces. These surfaces are then skeletonized for rendering and camera position tracking. Finally, we train a style transfer model using Contrastive Unpaired Translation (CUT) to synthesize realistic ureteroscopy images. Cross validation on the UNet model achieved a Dice score of 0.853 \${\textbackslash}pm\$ 0.084 for the CT segmentation step. CUT style transfer produced visually plausible images; the Kernel Inception Distance to real ureteroscopy images was reduced from 0.198 (rendered) to 0.089 (synthesized). We also qualitatively demonstrate the entire pipeline from CT to synthesized ureteroscopy. The proposed ASSIST-U system shows promise for aiding surgeons in visualization of kidney ureteroscopy.},
	language = {en},
	urldate = {2023-12-14},
	institution = {Preprints},
	author = {Lu, Daiwei and Wu, Yifan and Acar, Ayberk and Yao, Xing and Wu, Jie Ying and Kavoussi, Nicholas and Oguz, Ipek},
	month = nov,
	year = {2023},
	doi = {10.22541/au.169960983.39481168/v1},
	file = {Lu et al. - 2023 - ASSIST-U A System for Segmentation and Image Styl.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\QLCL3NV4\\Lu et al. - 2023 - ASSIST-U A System for Segmentation and Image Styl.pdf:application/pdf},
}

@article{wu_cross-modal_2021-1,
	title = {Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery},
	volume = {16},
	issn = {1861-6410, 1861-6429},
	url = {https://link.springer.com/10.1007/s11548-021-02343-y},
	doi = {10.1007/s11548-021-02343-y},
	abstract = {Purpose Multi- and cross-modal learning consolidates information from multiple data sources which may offer a holistic representation of complex scenarios. Cross-modal learning is particularly interesting, because synchronized data streams are immediately useful as self-supervisory signals. The prospect of achieving self-supervised continual learning in surgical robotics is exciting as it may enable lifelong learning that adapts to different surgeons and cases, ultimately leading to a more general machine understanding of surgical processes.
Methods We present a learning paradigm using synchronous video and kinematics from robot-mediated surgery. Our approach relies on an encoder–decoder network that maps optical ﬂow to the corresponding kinematics sequence. Clustering on the latent representations reveals meaningful groupings for surgeon gesture and skill level. We demonstrate the generalizability of the representations on the JIGSAWS dataset by classifying skill and gestures on tasks not used for training.
Results For tasks seen in training, we report a 59 to 70\% accuracy in surgical gestures classiﬁcation. On tasks beyond the training setup, we note a 45 to 65\% accuracy. Qualitatively, we ﬁnd that unseen gestures form clusters in the latent space of novice actions, which may enable the automatic identiﬁcation of novel interactions in a lifelong learning scenario.
Conclusion From predicting the synchronous kinematics sequence, optical ﬂow representations of surgical scenes emerge that separate well even for new tasks that the model had not seen before. While the representations are useful immediately for a variety of tasks, the self-supervised learning paradigm may enable research in lifelong and user-speciﬁc learning.},
	language = {en},
	number = {5},
	urldate = {2023-12-14},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Wu, Jie Ying and Tamhane, Aniruddha and Kazanzides, Peter and Unberath, Mathias},
	month = may,
	year = {2021},
	pages = {779--787},
	file = {Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\V7UISSZQ\\Wu et al. - 2021 - Cross-modal self-supervised representation learnin.pdf:application/pdf},
}

@misc{karras_dreampose_2023,
	title = {{DreamPose}: {Fashion} {Image}-to-{Video} {Synthesis} via {Stable} {Diffusion}},
	shorttitle = {{DreamPose}},
	url = {http://arxiv.org/abs/2304.06025},
	abstract = {We present DreamPose, a diffusion-based method for generating animated fashion videos from still images. Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pretrained text-to-image model (Stable Diffusion) into a pose-and-image guided video synthesis model, using a novel fine-tuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We fine-tune on a collection of fashion videos from the UBC Fashion dataset. We evaluate our method on a variety of clothing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video animation.Video results are available on our project page.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Karras, Johanna and Holynski, Aleksander and Wang, Ting-Chun and Kemelmacher-Shlizerman, Ira},
	month = oct,
	year = {2023},
	note = {arXiv:2304.06025 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project page: https://grail.cs.washington.edu/projects/dreampose/},
	file = {Karras et al. - 2023 - DreamPose Fashion Image-to-Video Synthesis via St.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\8QZ6LFNK\\Karras et al. - 2023 - DreamPose Fashion Image-to-Video Synthesis via St.pdf:application/pdf},
}

@misc{zhu_tryondiffusion_2023,
	title = {{TryOnDiffusion}: {A} {Tale} of {Two} {UNets}},
	shorttitle = {{TryOnDiffusion}},
	url = {http://arxiv.org/abs/2306.08276},
	abstract = {Given two images depicting a person and a garment worn by another person, our goal is to generate a visualization of how the garment might look on the input person. A key challenge is to synthesize a photorealistic detailpreserving visualization of the garment, while warping the garment to accommodate a significant body pose and shape change across the subjects. Previous methods either focus on garment detail preservation without effective pose 1Work done while author was an intern at Google.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhu, Luyang and Yang, Dawei and Zhu, Tyler and Reda, Fitsum and Chan, William and Saharia, Chitwan and Norouzi, Mohammad and Kemelmacher-Shlizerman, Ira},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08276 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: CVPR 2023. Project page: https://tryondiffusion.github.io/},
	file = {Zhu et al. - 2023 - TryOnDiffusion A Tale of Two UNets.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\AEJ4N3H3\\Zhu et al. - 2023 - TryOnDiffusion A Tale of Two UNets.pdf:application/pdf},
}

@article{nofallah_improving_2022,
	title = {Improving the {Diagnosis} of {Skin} {Biopsies} {Using} {Tissue} {Segmentation}},
	volume = {12},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/12/7/1713},
	doi = {10.3390/diagnostics12071713},
	abstract = {Invasive melanoma, a common type of skin cancer, is considered one of the deadliest. Pathologists routinely evaluate melanocytic lesions to determine the amount of atypia, and if the lesion represents an invasive melanoma, its stage. However, due to the complicated nature of these assessments, inter- and intra-observer variability among pathologists in their interpretation are very common. Machine-learning techniques have shown impressive and robust performance on various tasks including healthcare. In this work, we study the potential of including semantic segmentation of clinically important tissue structure in improving the diagnosis of skin biopsy images. Our experimental results show a 6\% improvement in F-score when using whole slide images along with epidermal nests and cancerous dermal nest segmentation masks compared to using whole-slide images alone in training and testing the diagnosis pipeline.},
	language = {en},
	number = {7},
	urldate = {2023-12-14},
	journal = {Diagnostics},
	author = {Nofallah, Shima and Li, Beibin and Mokhtari, Mojgan and Wu, Wenjun and Knezevich, Stevan and May, Caitlin J. and Chang, Oliver H. and Elmore, Joann G. and Shapiro, Linda G.},
	month = jul,
	year = {2022},
	pages = {1713},
	file = {Nofallah et al. - 2022 - Improving the Diagnosis of Skin Biopsies Using Tis.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\NAGUR8ZV\\Nofallah et al. - 2022 - Improving the Diagnosis of Skin Biopsies Using Tis.pdf:application/pdf},
}

@inproceedings{liu_vsgd-net_2023,
	address = {Waikoloa, HI, USA},
	title = {{VSGD}-{Net}: {Virtual} {Staining} {Guided} {Melanocyte} {Detection} on {Histopathological} {Images}},
	isbn = {978-1-66549-346-8},
	shorttitle = {{VSGD}-{Net}},
	url = {https://ieeexplore.ieee.org/document/10030364/},
	doi = {10.1109/WACV56688.2023.00196},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2023 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Liu, Kechun and Li, Beibin and Wu, Wenjun and May, Caitlin and Chang, Oliver and Knezevich, Stevan and Reisch, Lisa and Elmore, Joann and Shapiro, Linda},
	month = jan,
	year = {2023},
	pages = {1918--1927},
	file = {Liu et al. - 2023 - VSGD-Net Virtual Staining Guided Melanocyte Detec.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\MGQZG9TY\\Liu et al. - 2023 - VSGD-Net Virtual Staining Guided Melanocyte Detec.pdf:application/pdf},
}

@article{nofallah_segmenting_2022,
	title = {Segmenting {Skin} {Biopsy} {Images} with {Coarse} and {Sparse} {Annotations} using {U}-{Net}},
	volume = {35},
	issn = {0897-1889, 1618-727X},
	url = {https://link.springer.com/10.1007/s10278-022-00641-8},
	doi = {10.1007/s10278-022-00641-8},
	abstract = {The number of melanoma diagnoses has increased dramatically over the past three decades, outpacing almost all other cancers. Nearly 1 in 4 skin biopsies is of melanocytic lesions, highlighting the clinical and public health importance of correct diagnosis. Deep learning image analysis methods may improve and complement current diagnostic and prognostic capabilities. The histologic evaluation of melanocytic lesions, including melanoma and its precursors, involves determining whether the melanocytic population involves the epidermis, dermis, or both. Semantic segmentation of clinically important structures in skin biopsies is a crucial step towards an accurate diagnosis. While training a segmentation model requires ground-truth labels, annotation of large images is a labor-intensive task. This issue becomes especially pronounced in a medical image dataset in which expert annotation is the gold standard. In this paper, we propose a two-stage segmentation pipeline using coarse and sparse annotations on a small region of the whole slide image as the training set. Segmentation results on whole slide images show promising performance for the proposed pipeline.},
	language = {en},
	number = {5},
	urldate = {2023-12-14},
	journal = {Journal of Digital Imaging},
	author = {Nofallah, Shima and Mokhtari, Mojgan and Wu, Wenjun and Mehta, Sachin and Knezevich, Stevan and May, Caitlin J. and Chang, Oliver H. and Lee, Annie C. and Elmore, Joann G. and Shapiro, Linda G.},
	month = oct,
	year = {2022},
	pages = {1238--1249},
	file = {Nofallah et al. - 2022 - Segmenting Skin Biopsy Images with Coarse and Spar.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\CA8X9WAF\\Nofallah et al. - 2022 - Segmenting Skin Biopsy Images with Coarse and Spar.pdf:application/pdf},
}

@techreport{kim_fostering_2023,
	type = {preprint},
	title = {Fostering transparent medical image {AI} via an image-text foundation model grounded in medical literature},
	url = {http://medrxiv.org/lookup/doi/10.1101/2023.06.07.23291119},
	abstract = {Abstract
          
            Building trustworthy and transparent image-based medical AI systems requires the ability to interrogate data and models at all stages of the development pipeline: from training models to post-deployment monitoring. Ideally, the data and associated AI systems could be described using terms already familiar to physicians, but this requires medical datasets densely annotated with semantically meaningful concepts. Here, we present a foundation model approach, named MONET (
            M
            edical c
            ON
            cept r
            ET
            riever), which learns how to connect medical images with text and generates dense concept annotations to enable tasks in AI transparency from model auditing to model interpretation. Dermatology provides a demanding use case for the versatility of MONET, due to the heterogeneity in diseases, skin tones, and imaging modalities. We trained MONET on the basis of 105,550 dermatological images paired with natural language descriptions from a large collection of medical literature. MONET can accurately annotate concepts across dermatology images as verified by board-certified dermatologists, outperforming supervised models built on previously concept-annotated dermatology datasets. We demonstrate how MONET enables AI transparency across the entire AI development pipeline from dataset auditing to model auditing to building inherently interpretable models.},
	language = {en},
	urldate = {2023-12-14},
	institution = {Dermatology},
	author = {Kim, Chanwoo and Gadgil, Soham U. and DeGrave, Alex J. and Cai, Zhuo Ran and Daneshjou, Roxana and Lee, Su-In},
	month = jun,
	year = {2023},
	doi = {10.1101/2023.06.07.23291119},
	file = {Kim et al. - 2023 - Fostering transparent medical image AI via an imag.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\P8T99ARE\\Kim et al. - 2023 - Fostering transparent medical image AI via an imag.pdf:application/pdf},
}

@misc{covert_learning_2023,
	title = {Learning to {Estimate} {Shapley} {Values} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2206.05282},
	abstract = {Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model’s dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we ﬁrst leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we ﬁnd that our approach provides more accurate explanations than existing methods for ViTs.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Covert, Ian and Kim, Chanwoo and Lee, Su-In},
	month = mar,
	year = {2023},
	note = {arXiv:2206.05282 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICLR 2023 camera-ready},
	file = {Covert et al. - 2023 - Learning to Estimate Shapley Values with Vision Tr.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\RC39Q6FI\\Covert et al. - 2023 - Learning to Estimate Shapley Values with Vision Tr.pdf:application/pdf},
}

@techreport{beebe-wang_md-ad_2018,
	type = {preprint},
	title = {{MD}-{AD}: {Multi}-task deep learning for {Alzheimer}’s disease neuropathology},
	shorttitle = {{MD}-{AD}},
	url = {http://biorxiv.org/lookup/doi/10.1101/331942},
	abstract = {Systematic modeling of Alzheimer's Disease (AD) neuropathology based on brain gene expression would provide valuable insights into the disease. However, relative scarcity and regional heterogeneity of brain gene expression and neuropathology datasets obscure the ability to robustly identify expression markers. We propose MD-AD (Multi-task Deep learning for AD) to effectively combine heterogeneous AD datasets by simultaneously modeling multiple phenotypes with shared layers. MD-AD leads to an 8\% and 5\% reduction in mean squared error over MLP for predicting counts of two AD hallmarks: plaques and tangles. It also leads to a 40\% and 30\% reduction in classiﬁcation error over MLP for two common staging systems for AD: CERAD score and Braak stage. Additionally, MD-AD's network representation tends to better capture known metabolic pathways, including some AD-related pathways. Together, these results indicate that MD-AD is particularly useful for learning expressive network representations from heterogeneous and sparsely labeled AD data.},
	language = {en},
	urldate = {2023-12-14},
	institution = {Systems Biology},
	author = {Beebe-Wang, Nicasia and Celik, Safiye and Lee, Su-In},
	month = may,
	year = {2018},
	doi = {10.1101/331942},
	file = {Beebe-Wang et al. - 2018 - MD-AD Multi-task deep learning for Alzheimer’s di.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\TXHA9CSQ\\Beebe-Wang et al. - 2018 - MD-AD Multi-task deep learning for Alzheimer’s di.pdf:application/pdf},
}

@misc{wang_boosting_2022,
	title = {Boosting {Active} {Learning} via {Improving} {Test} {Performance}},
	url = {http://arxiv.org/abs/2112.05683},
	abstract = {Central to active learning (AL) is what data should be selected for annotation. Existing works attempt to select highly uncertain or informative data for annotation. Nevertheless, it remains unclear how selected data impacts the test performance of the task model used in AL. In this work, we explore such an impact by theoretically proving that selecting unlabeled data of higher gradient norm leads to a lower upper-bound of test loss, resulting in a better test performance. However, due to the lack of label information, directly computing gradient norm for unlabeled data is infeasible. To address this challenge, we propose two schemes, namely expected-gradnorm and entropy-gradnorm. The former computes the gradient norm by constructing an expected empirical loss while the latter constructs an unsupervised loss with entropy. Furthermore, we integrate the two schemes in a universal AL framework. We evaluate our method on classical image classiﬁcation and semantic segmentation tasks. To demonstrate its competency in domain applications and its robustness to noise, we also validate our method on a cellular imaging analysis task, namely cryo-Electron Tomography subtomogram classiﬁcation. Results demonstrate that our method achieves superior performance against the state of the art. Our source code is available at https://github.com/xulabs/aitom/blob/master/doc/projects/ al gradnorm.md.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wang, Tianyang and Li, Xingjian and Yang, Pengkun and Hu, Guosheng and Zeng, Xiangrui and Huang, Siyu and Xu, Cheng-Zhong and Xu, Min},
	month = jan,
	year = {2022},
	note = {arXiv:2112.05683 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 13 pages},
	file = {Wang et al. - 2022 - Boosting Active Learning via Improving Test Perfor.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\48QXFYFI\\Wang et al. - 2022 - Boosting Active Learning via Improving Test Perfor.pdf:application/pdf},
}

@inproceedings{zeng_end--end_2021,
	address = {Montreal, QC, Canada},
	title = {End-to-end robust joint unsupervised image alignment and clustering},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710053/},
	doi = {10.1109/ICCV48922.2021.00383},
	abstract = {Computing dense pixel-to-pixel image correspondences is a fundamental task of computer vision. Often, the objective is to align image pairs from the same semantic category for manipulation or segmentation purposes. Despite achieving superior performance, existing deep learning alignment methods cannot cluster images; consequently, clustering and pairing images needed to be a separate laborious and expensive step.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zeng, Xiangrui and Howe, Gregory and Xu, Min},
	month = oct,
	year = {2021},
	pages = {3834--3846},
	file = {Zeng et al. - 2021 - End-to-end robust joint unsupervised image alignme.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\637MCVUE\\Zeng et al. - 2021 - End-to-end robust joint unsupervised image alignme.pdf:application/pdf},
}

@inproceedings{uddin_harmony_2022,
	address = {New Orleans, LA, USA},
	title = {Harmony: {A} {Generic} {Unsupervised} {Approach} for {Disentangling} {Semantic} {Content} from {Parameterized} {Transformations}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Harmony},
	url = {https://ieeexplore.ieee.org/document/9880312/},
	doi = {10.1109/CVPR52688.2022.01999},
	abstract = {In many real-life image analysis applications, particularly in biomedical research domains, the objects of interest undergo multiple transformations that alters their visual properties while keeping the semantic content unchanged. Disentangling images into semantic content factors and transformations can provide significant benefits into many domain-specific image analysis tasks. To this end, we propose a generic unsupervised framework, Harmony, that simultaneously and explicitly disentangles semantic content from multiple parameterized transformations. Harmony leverages a simple cross-contrastive learning framework with multiple explicitly parameterized latent representations to disentangle content from transformations. To demonstrate the efficacy of Harmony, we apply it to disentangle image semantic content from several parameterized transformations (rotation, translation, scaling, and contrast). Harmony achieves significantly improved disentanglement over the baseline models on several image datasets of diverse domains. With such disentanglement, Harmony is demonstrated to incentivize bioimage analysis research by modeling structural heterogeneity of macromolecules from cryo-ET images and learning transformation-invariant representations of protein particles from single-particle cryoEM images. Harmony also performs very well in disentangling content from 3D transformations and can perform coarse and fast alignment of 3D cryo-ET subtomograms. Therefore, Harmony is generalizable to many other imaging domains and can potentially be extended to domains beyond imaging as well.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Uddin, Mostofa Rafid and Howe, Gregory and Zeng, Xiangrui and Xu, Min},
	month = jun,
	year = {2022},
	pages = {20614--20623},
	file = {Uddin et al. - 2022 - Harmony A Generic Unsupervised Approach for Disen.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\5QJVWKBH\\Uddin et al. - 2022 - Harmony A Generic Unsupervised Approach for Disen.pdf:application/pdf},
}

@inproceedings{zhu_weakly_2021,
	address = {Montreal, QC, Canada},
	title = {Weakly {Supervised} {3D} {Semantic} {Segmentation} {Using} {Cross}-{Image} {Consensus} and {Inter}-{Voxel} {Affinity} {Relations}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710126/},
	doi = {10.1109/ICCV48922.2021.00283},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Xiaoyu and Chen, Jeffrey and Zeng, Xiangrui and Liang, Junwei and Li, Chengqi and Liu, Sinuo and Behpour, Sima and Xu, Min},
	month = oct,
	year = {2021},
	pages = {2814--2824},
	file = {Zhu et al. - 2021 - Weakly Supervised 3D Semantic Segmentation Using C.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\CKLWFS7W\\Zhu et al. - 2021 - Weakly Supervised 3D Semantic Segmentation Using C.pdf:application/pdf},
}

@inproceedings{zeng_gum-net_2020,
	address = {Seattle, WA, USA},
	title = {Gum-{Net}: {Unsupervised} {Geometric} {Matching} for {Fast} and {Accurate} {3D} {Subtomogram} {Image} {Alignment} and {Averaging}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Gum-{Net}},
	url = {https://ieeexplore.ieee.org/document/9156695/},
	doi = {10.1109/CVPR42600.2020.00413},
	abstract = {We propose a Geometric unsupervised matching Network (Gum-Net) for ﬁnding the geometric correspondence between two images with application to 3D subtomogram alignment and averaging. Subtomogram alignment is the most important task in cryo-electron tomography (cryo-ET), a revolutionary 3D imaging technique for visualizing the molecular organization of unperturbed cellular landscapes in single cells. However, subtomogram alignment and averaging are very challenging due to severe imaging limits such as noise and missing wedge effects. We introduce an end-to-end trainable architecture with three novel modules speciﬁcally designed for preserving feature spatial information and propagating feature matching information. The training is performed in a fully unsupervised fashion to optimize a matching metric. No ground truth transformation information nor category-level or instance-level matching supervision information is needed. After systematic assessments on six real and nine simulated datasets, we demonstrate that Gum-Net reduced the alignment error by 40 to 50\% and improved the averaging resolution by 10\%. GumNet also achieved 70 to 110 times speedup in practice with GPU acceleration compared to state-of-the-art subtomogram alignment methods. Our work is the ﬁrst 3D unsupervised geometric matching method for images of strong transformation variation and high noise level. The training code, trained model, and datasets are available in our open-source software AITom 1.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zeng, Xiangrui and Xu, Min},
	month = jun,
	year = {2020},
	pages = {4072--4083},
	file = {Zeng and Xu - 2020 - Gum-Net Unsupervised Geometric Matching for Fast .pdf:C\:\\Users\\lexieli\\Zotero\\storage\\6JW59XMZ\\Zeng and Xu - 2020 - Gum-Net Unsupervised Geometric Matching for Fast .pdf:application/pdf},
}

@misc{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv:2005.12872 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\4ZADZH3B\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2023 - Attention Is All You Need.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\843X7ST7\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in lowdimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-speciﬁc Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	language = {en},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Project page: https://people.eecs.berkeley.edu/{\textasciitilde}bmild/fourfeat/},
	file = {Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\MTR6NDW6\\Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf:application/pdf},
}

@article{li_phrase-level_2022,
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	abstract = {Video temporal localization aims to locate a period that semantically matches a natural language query in a given untrimmed video. We empirically observe that although existing approaches gain steady progress on sentence localization, the performance of phrase localization is far from satisfactory. In principle, the phrase should be easier to localize as fewer combinations of visual concepts need to be considered; such incapability indicates that the existing models only capture the sentence annotation bias in the benchmark but lack sufficient understanding of the intrinsic relationship between simple visual and language concepts, thus the model generalization and interpretability is questioned. This paper proposes a unified framework that can deal with both sentence and phrase-level localization, namely Phrase Level Prediction Net (PLPNet). Specifically, based on the hypothesis that similar phrases tend to focus on similar video cues, while dissimilar ones should not, we build a contrastive mechanism to restrain phrase-level localization without fine-grained phrase boundary annotation required in training. Moreover, considering the sentence’s flexibility and wide discrepancy among phrases, we propose a clustering-based batch sampler to ensure that contrastive learning can be conducted efficiently. Extensive experiments demonstrate that our method surpasses state-of-the-art methods of phrase-level temporal localization while maintaining high performance in sentence localization and boosting the model’s interpretability and generalization capability. Our code is available at https://github.com/sizhelee/PLPNet.},
	language = {en},
	author = {Li, Sizhe},
	year = {2022},
	file = {Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\GDKD75RM\\Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:application/pdf},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\UU54AZB6\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{zhang_fm-ov3d_nodate,
	title = {{FM}-{OV3D}: {Foundation} {Model}-based {Cross}-modal {Knowledge} {Blending} for {Open}-{Vocabulary} {3D} {Detection}},
	abstract = {The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D models’ open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pretrained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.},
	language = {en},
	author = {Zhang, Dongmei and Li, Chang and Zhang, Renrui and Xie, Shenghao and Xue, Wei and Xie, Xiaodong and Zhang, Shanghang},
	file = {Zhang et al. - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\EHTXBH9T\\Zhang et al. - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:application/pdf},
}

@misc{qi_deep_2019,
	title = {Deep {Hough} {Voting} for {3D} {Object} {Detection} in {Point} {Clouds}},
	url = {http://arxiv.org/abs/1904.09664},
	abstract = {Current 3D object detection methods are heavily inﬂuenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird’s eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to ﬁrst principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data – samples from 2D manifolds in 3D space – we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efﬁciency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.},
	language = {en},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Qi, Charles R. and Litany, Or and He, Kaiming and Guibas, Leonidas J.},
	month = aug,
	year = {2019},
	note = {arXiv:1904.09664 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2019},
	file = {Qi et al. - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\882JYYMH\\Qi et al. - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf:application/pdf},
}

@misc{qi_pointnet_2017-1,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	language = {en},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\RG8ZM4FF\\Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf},
}

@article{li_possibility_nodate,
	title = {The {Possibility} of {Developing} a {Universal} {Algorithm} for {All} {Medical} {Image} {Segmentation} {Tasks}},
	abstract = {Deep learning has played an essential part in the process of technological development. With assistance from deep learning algorithms, AI agents can perform like humans in various tasks, leading to the research towards applying deep learning to other ﬁelds. The development in medical imaging devices has provided researchers plenty of medical images. Both factors led to technological developments in the medical image segmentation ﬁeld. However, there are many barriers to the research in medical image segmentation algorithms. After introducing the background and basic knowledge of medical image segmentation tasks, we provide a new way of categorizing these challenges based on these datasets’ scale. This dissertation also discusses four challenges that the research of medical image segmentation algorithms faces, in the sequence of data access, data annotation, deep learning models’ application, and the variance of datasets, in aspects of ethics and technology. Then, we discuss the probability of developing a universal algorithm that can achieve state-of-the-art performances on all datasets by analyzing previous works. We also conducted experiments based on four powerful 2D image segmentation models and two medical image datasets to prove our opinion. At last, we propose a future working direction for developing a universal algorithm in the medical image segmentation ﬁeld.},
	language = {en},
	author = {Li, Chang},
	file = {Li - The Possibility of Developing a Universal Algorith.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\FANZ7IWR\\Li - The Possibility of Developing a Universal Algorith.pdf:application/pdf},
}

@inproceedings{li_phrase-level_2022-1,
	address = {Newark NJ USA},
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	isbn = {978-1-4503-9238-9},
	url = {https://dl.acm.org/doi/10.1145/3512527.3531382},
	doi = {10.1145/3512527.3531382},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Li, Sizhe and Li, Chang and Zheng, Minghang and Liu, Yang},
	month = jun,
	year = {2022},
	pages = {360--368},
}

@article{li_phrase-level_2022-2,
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	abstract = {Video temporal localization aims to locate a period that semantically matches a natural language query in a given untrimmed video. We empirically observe that although existing approaches gain steady progress on sentence localization, the performance of phrase localization is far from satisfactory. In principle, the phrase should be easier to localize as fewer combinations of visual concepts need to be considered; such incapability indicates that the existing models only capture the sentence annotation bias in the benchmark but lack sufficient understanding of the intrinsic relationship between simple visual and language concepts, thus the model generalization and interpretability is questioned. This paper proposes a unified framework that can deal with both sentence and phrase-level localization, namely Phrase Level Prediction Net (PLPNet). Specifically, based on the hypothesis that similar phrases tend to focus on similar video cues, while dissimilar ones should not, we build a contrastive mechanism to restrain phrase-level localization without fine-grained phrase boundary annotation required in training. Moreover, considering the sentence’s flexibility and wide discrepancy among phrases, we propose a clustering-based batch sampler to ensure that contrastive learning can be conducted efficiently. Extensive experiments demonstrate that our method surpasses state-of-the-art methods of phrase-level temporal localization while maintaining high performance in sentence localization and boosting the model’s interpretability and generalization capability. Our code is available at https://github.com/sizhelee/PLPNet.},
	language = {en},
	author = {Li, Sizhe},
	year = {2022},
	file = {Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\5PCKZNB5\\Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:application/pdf},
}

@misc{zhang_fm-ov3d_2023,
	title = {{FM}-{OV3D}: {Foundation} {Model}-based {Cross}-modal {Knowledge} {Blending} for {Open}-{Vocabulary} {3D} {Detection}},
	shorttitle = {{FM}-{OV3D}},
	url = {http://arxiv.org/abs/2312.14465},
	abstract = {The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D models' open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pre-trained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Dongmei and Li, Chang and Zhang, Ray and Xie, Shenghao and Xue, Wei and Xie, Xiaodong and Zhang, Shanghang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14465 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI 2024. Code will be released at https://github.com/dmzhang0425/FM-OV3D.git},
	file = {arXiv Fulltext PDF:C\:\\Users\\lexieli\\Zotero\\storage\\MUUZJ3YR\\Zhang et al. - 2023 - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lexieli\\Zotero\\storage\\QLBNYNHH\\2312.html:text/html},
}
