
@inproceedings{li_phrase-level_2022,
	address = {Newark NJ USA},
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	isbn = {978-1-4503-9238-9},
	url = {https://dl.acm.org/doi/10.1145/3512527.3531382},
	doi = {10.1145/3512527.3531382},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Li, Sizhe and Li, Chang and Zheng, Minghang and Liu, Yang},
	month = jun,
	year = {2022},
	pages = {360--368},
}

@article{li_phrase-level_2022-1,
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	abstract = {Video temporal localization aims to locate a period that semantically matches a natural language query in a given untrimmed video. We empirically observe that although existing approaches gain steady progress on sentence localization, the performance of phrase localization is far from satisfactory. In principle, the phrase should be easier to localize as fewer combinations of visual concepts need to be considered; such incapability indicates that the existing models only capture the sentence annotation bias in the benchmark but lack sufficient understanding of the intrinsic relationship between simple visual and language concepts, thus the model generalization and interpretability is questioned. This paper proposes a unified framework that can deal with both sentence and phrase-level localization, namely Phrase Level Prediction Net (PLPNet). Specifically, based on the hypothesis that similar phrases tend to focus on similar video cues, while dissimilar ones should not, we build a contrastive mechanism to restrain phrase-level localization without fine-grained phrase boundary annotation required in training. Moreover, considering the sentence’s flexibility and wide discrepancy among phrases, we propose a clustering-based batch sampler to ensure that contrastive learning can be conducted efficiently. Extensive experiments demonstrate that our method surpasses state-of-the-art methods of phrase-level temporal localization while maintaining high performance in sentence localization and boosting the model’s interpretability and generalization capability. Our code is available at https://github.com/sizhelee/PLPNet.},
	language = {en},
	author = {Li, Sizhe},
	year = {2022},
	file = {Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\5PCKZNB5\\Li - 2022 - Phrase-level Prediction for Video Temporal Localiz.pdf:application/pdf},
}

@misc{zhang_fm-ov3d_2023,
	title = {{FM}-{OV3D}: {Foundation} {Model}-based {Cross}-modal {Knowledge} {Blending} for {Open}-{Vocabulary} {3D} {Detection}},
	shorttitle = {{FM}-{OV3D}},
	url = {http://arxiv.org/abs/2312.14465},
	abstract = {The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D models' open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pre-trained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Dongmei and Li, Chang and Zhang, Ray and Xie, Shenghao and Xue, Wei and Xie, Xiaodong and Zhang, Shanghang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14465 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI 2024. Code will be released at https://github.com/dmzhang0425/FM-OV3D.git},
	file = {arXiv Fulltext PDF:C\:\\Users\\lexieli\\Zotero\\storage\\MUUZJ3YR\\Zhang et al. - 2023 - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lexieli\\Zotero\\storage\\QLBNYNHH\\2312.html:text/html},
}
