
@misc{zhang_fm-ov3d_2023,
	title = {{FM}-{OV3D}: {Foundation} {Model}-based {Cross}-modal {Knowledge} {Blending} for {Open}-{Vocabulary} {3D} {Detection}},
	shorttitle = {{FM}-{OV3D}},
	url = {http://arxiv.org/abs/2312.14465},
	abstract = {The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D modelsâ€™ open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pretrained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.},
	language = {en},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Dongmei and Li, Chang and Zhang, Ray and Xie, Shenghao and Xue, Wei and Xie, Xiaodong and Zhang, Shanghang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14465 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI 2024. Code will be released at https://github.com/dmzhang0425/FM-OV3D.git},
	file = {Zhang et al. - 2023 - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:C\:\\Users\\lexieli\\Zotero\\storage\\GYAKFG8L\\Zhang et al. - 2023 - FM-OV3D Foundation Model-based Cross-modal Knowle.pdf:application/pdf},
}

@inproceedings{li_phrase-level_2022,
	address = {Newark NJ USA},
	title = {Phrase-level {Prediction} for {Video} {Temporal} {Localization}},
	isbn = {978-1-4503-9238-9},
	url = {https://dl.acm.org/doi/10.1145/3512527.3531382},
	doi = {10.1145/3512527.3531382},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Li, Sizhe and Li, Chang and Zheng, Minghang and Liu, Yang},
	month = jun,
	year = {2022},
	pages = {360--368},
}
